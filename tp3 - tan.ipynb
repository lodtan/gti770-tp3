{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from IPython.display import Markdown\n",
    "from tensorflow import keras\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from time import time\n",
    "from sklearn.preprocessing import normalize\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = pd.read_csv('galaxy_feature_vectors.csv', delimiter = ',', header=None)\n",
    "labels = pd.read_csv('galaxy_label_data_set.csv', delimiter = ',')\n",
    "X_galaxy = pd.read_csv('galaxy_feature_vectors.csv', delimiter = ',', header=None).values[:,0:-1]\n",
    "Y_galaxy = pd.read_csv('galaxy_feature_vectors.csv', delimiter = ',', header=None).values[:,-1:].astype(int).flatten()\n",
    "Xg_train, Xg_test, Yg_train, Yg_test = train_test_split(X_galaxy, Y_galaxy, test_size=0.20, random_state=42, stratify=Y_galaxy)\n",
    "\n",
    "# On normalise nos features\n",
    "scaler = StandardScaler()\n",
    "Xg_train = scaler.fit_transform(Xg_train)\n",
    "Xg_test = scaler.fit_transform(Xg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'array pour stocker l'accuracy et le score f1 pour les différents nombres d'itération\n",
    "accuracies = []\n",
    "f1_scores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_npercep = []\n",
    "f1_scores_npercep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_layers = []\n",
    "f1_scores_layers = []\n",
    "accuracies_learning = []\n",
    "f1_scores_learning = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Parameters\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 60\n",
    "batch = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 100 # 1st layer number of neurons\n",
    "n_hidden_2 = 100 # 2nd layer number of neurons\n",
    "n_hidden_3 = 50 # 3rd layer number of neurons\n",
    "n_hidden_4 = 2 # 3rd layer number of neurons\n",
    "num_input = 75\n",
    "num_classes = 2\n",
    "\"\"\"\n",
    "batch = 100\n",
    "\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 60\n",
    "n_hidden_1 = 100 # 1st layer number of neurons\n",
    "n_hidden_2 = 100 # 2nd layer number of neurons\n",
    "n_hidden_3 = 50 # 3rd layer number of neurons\n",
    "n_hidden_4 = 2 # 4th layer number of neurons\n",
    "num_input = 75\n",
    "num_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre d’itérations (epochs)\n",
    "#n_epochs = 400\n",
    "learning_rate = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(n_hidden_1))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(n_hidden_2))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13526 samples, validate on 3382 samples\n",
      "Epoch 1/60\n",
      "13526/13526 [==============================] - 2s 172us/step - loss: 0.5395 - acc: 0.8961 - val_loss: 0.1539 - val_acc: 0.9456\n",
      "Epoch 2/60\n",
      "13526/13526 [==============================] - 2s 123us/step - loss: 0.1512 - acc: 0.9444 - val_loss: 0.1516 - val_acc: 0.9435\n",
      "Epoch 3/60\n",
      "13526/13526 [==============================] - 2s 116us/step - loss: 0.1425 - acc: 0.9453 - val_loss: 0.1473 - val_acc: 0.9453\n",
      "Epoch 4/60\n",
      "13526/13526 [==============================] - 2s 120us/step - loss: 0.1316 - acc: 0.9499 - val_loss: 0.1222 - val_acc: 0.9551\n",
      "Epoch 5/60\n",
      "13526/13526 [==============================] - 2s 120us/step - loss: 0.1257 - acc: 0.9518 - val_loss: 0.9328 - val_acc: 0.5724\n",
      "Epoch 6/60\n",
      "13526/13526 [==============================] - 2s 127us/step - loss: 0.1546 - acc: 0.9446 - val_loss: 0.1810 - val_acc: 0.9293\n",
      "Epoch 7/60\n",
      "13526/13526 [==============================] - 2s 122us/step - loss: 0.1133 - acc: 0.9582 - val_loss: 0.1603 - val_acc: 0.9441\n",
      "Epoch 8/60\n",
      "13526/13526 [==============================] - 2s 116us/step - loss: 0.1076 - acc: 0.9593 - val_loss: 0.0975 - val_acc: 0.9654\n",
      "Epoch 9/60\n",
      "13526/13526 [==============================] - 2s 115us/step - loss: 0.1142 - acc: 0.9555 - val_loss: 0.2116 - val_acc: 0.9101\n",
      "Epoch 10/60\n",
      "13526/13526 [==============================] - 2s 118us/step - loss: 0.1107 - acc: 0.9582 - val_loss: 0.5218 - val_acc: 0.8223\n",
      "Epoch 11/60\n",
      "13526/13526 [==============================] - 2s 119us/step - loss: 0.1150 - acc: 0.9556 - val_loss: 0.1505 - val_acc: 0.9483\n",
      "Epoch 12/60\n",
      "13526/13526 [==============================] - 2s 127us/step - loss: 0.1029 - acc: 0.9622 - val_loss: 0.1393 - val_acc: 0.9465\n",
      "Epoch 13/60\n",
      "13526/13526 [==============================] - 2s 127us/step - loss: 0.0972 - acc: 0.9630 - val_loss: 0.1202 - val_acc: 0.9554\n",
      "Epoch 14/60\n",
      "13526/13526 [==============================] - 2s 129us/step - loss: 0.0994 - acc: 0.9642 - val_loss: 0.1261 - val_acc: 0.9548\n",
      "Epoch 15/60\n",
      "13526/13526 [==============================] - 2s 127us/step - loss: 0.1002 - acc: 0.9619 - val_loss: 0.2627 - val_acc: 0.8989\n",
      "Epoch 16/60\n",
      "13526/13526 [==============================] - 2s 135us/step - loss: 0.0941 - acc: 0.9641 - val_loss: 0.1044 - val_acc: 0.9592\n",
      "Epoch 17/60\n",
      "13526/13526 [==============================] - 2s 132us/step - loss: 0.0931 - acc: 0.9645 - val_loss: 0.1100 - val_acc: 0.9616\n",
      "Epoch 18/60\n",
      "13526/13526 [==============================] - 2s 139us/step - loss: 0.0892 - acc: 0.9657 - val_loss: 0.2785 - val_acc: 0.9033\n",
      "Epoch 19/60\n",
      "13526/13526 [==============================] - 2s 128us/step - loss: 0.0890 - acc: 0.9660 - val_loss: 0.5060 - val_acc: 0.8714\n",
      "Epoch 20/60\n",
      "13526/13526 [==============================] - 2s 134us/step - loss: 0.0905 - acc: 0.9672 - val_loss: 0.0934 - val_acc: 0.9654\n",
      "Epoch 21/60\n",
      "13526/13526 [==============================] - 2s 139us/step - loss: 0.0876 - acc: 0.9671 - val_loss: 0.1056 - val_acc: 0.9657\n",
      "Epoch 22/60\n",
      "13526/13526 [==============================] - 2s 135us/step - loss: 0.0831 - acc: 0.9678 - val_loss: 0.1282 - val_acc: 0.9580\n",
      "Epoch 23/60\n",
      "13526/13526 [==============================] - 2s 145us/step - loss: 0.0820 - acc: 0.9687 - val_loss: 0.0903 - val_acc: 0.9687\n",
      "Epoch 24/60\n",
      "13526/13526 [==============================] - 2s 134us/step - loss: 0.0863 - acc: 0.9670 - val_loss: 0.1086 - val_acc: 0.9648\n",
      "Epoch 25/60\n",
      "13526/13526 [==============================] - 2s 134us/step - loss: 0.0822 - acc: 0.9686 - val_loss: 0.1645 - val_acc: 0.9429\n",
      "Epoch 26/60\n",
      "13526/13526 [==============================] - 2s 131us/step - loss: 0.0776 - acc: 0.9694 - val_loss: 0.1483 - val_acc: 0.9494\n",
      "Epoch 27/60\n",
      "13526/13526 [==============================] - 2s 129us/step - loss: 0.0779 - acc: 0.9700 - val_loss: 0.2254 - val_acc: 0.9219\n",
      "Epoch 28/60\n",
      "13526/13526 [==============================] - 2s 132us/step - loss: 0.0805 - acc: 0.9691 - val_loss: 0.0952 - val_acc: 0.9695\n",
      "Epoch 29/60\n",
      "13526/13526 [==============================] - 2s 136us/step - loss: 0.0769 - acc: 0.9706 - val_loss: 0.2718 - val_acc: 0.9270\n",
      "Epoch 30/60\n",
      "13526/13526 [==============================] - 2s 137us/step - loss: 0.0800 - acc: 0.9678 - val_loss: 0.3844 - val_acc: 0.9086\n",
      "Epoch 31/60\n",
      "13526/13526 [==============================] - 2s 129us/step - loss: 0.0752 - acc: 0.9706 - val_loss: 0.0976 - val_acc: 0.9675\n",
      "Epoch 32/60\n",
      "13526/13526 [==============================] - 2s 129us/step - loss: 0.0776 - acc: 0.9692 - val_loss: 0.1561 - val_acc: 0.9509\n",
      "Epoch 33/60\n",
      "13526/13526 [==============================] - 2s 137us/step - loss: 0.0761 - acc: 0.9708 - val_loss: 0.0995 - val_acc: 0.9666\n",
      "Epoch 34/60\n",
      "13526/13526 [==============================] - 2s 146us/step - loss: 0.0734 - acc: 0.9721 - val_loss: 0.1385 - val_acc: 0.9530\n",
      "Epoch 35/60\n",
      "13526/13526 [==============================] - 2s 131us/step - loss: 0.0709 - acc: 0.9740 - val_loss: 0.0975 - val_acc: 0.9704\n",
      "Epoch 36/60\n",
      "13526/13526 [==============================] - 2s 133us/step - loss: 0.0707 - acc: 0.9728 - val_loss: 0.1355 - val_acc: 0.9642\n",
      "Epoch 37/60\n",
      "13526/13526 [==============================] - 2s 133us/step - loss: 0.0746 - acc: 0.9714 - val_loss: 0.2625 - val_acc: 0.9181\n",
      "Epoch 38/60\n",
      "13526/13526 [==============================] - 2s 142us/step - loss: 0.0741 - acc: 0.9723 - val_loss: 0.1039 - val_acc: 0.9698\n",
      "Epoch 39/60\n",
      "13526/13526 [==============================] - 2s 129us/step - loss: 0.0683 - acc: 0.9751 - val_loss: 0.0873 - val_acc: 0.9695\n",
      "Epoch 40/60\n",
      "13526/13526 [==============================] - 2s 128us/step - loss: 0.0667 - acc: 0.9748 - val_loss: 0.2345 - val_acc: 0.9169\n",
      "Epoch 41/60\n",
      "13526/13526 [==============================] - 2s 133us/step - loss: 0.0656 - acc: 0.9761 - val_loss: 0.1256 - val_acc: 0.9607\n",
      "Epoch 42/60\n",
      "13526/13526 [==============================] - 2s 139us/step - loss: 0.0707 - acc: 0.9730 - val_loss: 0.1378 - val_acc: 0.9604\n",
      "Epoch 43/60\n",
      "13526/13526 [==============================] - 2s 141us/step - loss: 0.0693 - acc: 0.9731 - val_loss: 0.1156 - val_acc: 0.9648\n",
      "Epoch 44/60\n",
      "13526/13526 [==============================] - 2s 155us/step - loss: 0.0616 - acc: 0.9766 - val_loss: 0.1065 - val_acc: 0.9675\n",
      "Epoch 45/60\n",
      "13526/13526 [==============================] - 2s 141us/step - loss: 0.0631 - acc: 0.9759 - val_loss: 0.1167 - val_acc: 0.9630\n",
      "Epoch 46/60\n",
      "13526/13526 [==============================] - 2s 138us/step - loss: 0.0623 - acc: 0.9751 - val_loss: 0.1021 - val_acc: 0.9684\n",
      "Epoch 47/60\n",
      "13526/13526 [==============================] - 2s 139us/step - loss: 0.0614 - acc: 0.9773 - val_loss: 0.2509 - val_acc: 0.9329\n",
      "Epoch 48/60\n",
      "13526/13526 [==============================] - 2s 135us/step - loss: 0.0644 - acc: 0.9760 - val_loss: 0.1006 - val_acc: 0.9672\n",
      "Epoch 49/60\n",
      "13526/13526 [==============================] - 2s 149us/step - loss: 0.0623 - acc: 0.9766 - val_loss: 0.1189 - val_acc: 0.9645\n",
      "Epoch 50/60\n",
      "13526/13526 [==============================] - 2s 140us/step - loss: 0.0593 - acc: 0.9766 - val_loss: 0.0986 - val_acc: 0.9728\n",
      "Epoch 51/60\n",
      "13526/13526 [==============================] - 2s 137us/step - loss: 0.0572 - acc: 0.9789 - val_loss: 0.1521 - val_acc: 0.9551\n",
      "Epoch 52/60\n",
      "13526/13526 [==============================] - 2s 140us/step - loss: 0.0593 - acc: 0.9775 - val_loss: 0.1943 - val_acc: 0.9459\n",
      "Epoch 53/60\n",
      "13526/13526 [==============================] - 2s 144us/step - loss: 0.0660 - acc: 0.9753 - val_loss: 0.2739 - val_acc: 0.9151\n",
      "Epoch 54/60\n",
      "13526/13526 [==============================] - 2s 134us/step - loss: 0.0577 - acc: 0.9799 - val_loss: 0.1700 - val_acc: 0.9542\n",
      "Epoch 55/60\n",
      "13526/13526 [==============================] - 2s 134us/step - loss: 0.0614 - acc: 0.9771 - val_loss: 0.1155 - val_acc: 0.9672\n",
      "Epoch 56/60\n",
      "13526/13526 [==============================] - 2s 138us/step - loss: 0.0586 - acc: 0.9777 - val_loss: 0.2408 - val_acc: 0.9237\n",
      "Epoch 57/60\n",
      "13526/13526 [==============================] - 2s 134us/step - loss: 0.0585 - acc: 0.9777 - val_loss: 0.1151 - val_acc: 0.9684\n",
      "Epoch 58/60\n",
      "13526/13526 [==============================] - 2s 131us/step - loss: 0.0534 - acc: 0.9802 - val_loss: 0.2115 - val_acc: 0.9471\n",
      "Epoch 59/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13526/13526 [==============================] - 2s 127us/step - loss: 0.0555 - acc: 0.9795 - val_loss: 0.1835 - val_acc: 0.9548\n",
      "Epoch 60/60\n",
      "13526/13526 [==============================] - 2s 137us/step - loss: 0.0661 - acc: 0.9752 - val_loss: 0.3223 - val_acc: 0.9119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensorboard --logdir path_to_current_dir/Graph \\n'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = keras.callbacks.TensorBoard(log_dir='no_dropout/learning_3', batch_size=batch, histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "model.fit(Xg_train, Yg_train, epochs=n_epochs, batch_size=batch, validation_data=(Xg_test, Yg_test),\n",
    "          callbacks=[tensorboard])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "tensorboard --logdir path_to_current_dir/Graph \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3382/3382 [==============================] - 0s 83us/step\n",
      "Test accuracy: 0.9118864576115825\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(Xg_test, Yg_test)\n",
    "\n",
    "print('Test accuracy:', test_acc) \n",
    "accuracies_learning.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6667780e-12 1.0000000e+00]\n",
      " [9.9999940e-01 5.7977218e-07]\n",
      " [9.9999988e-01 8.8655788e-08]\n",
      " ...\n",
      " [1.0000000e+00 1.8156436e-10]\n",
      " [4.6510203e-04 9.9953496e-01]\n",
      " [9.3543941e-01 6.4560644e-02]]\n",
      "f1 score: 0.9116155733993972\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(Xg_test)\n",
    "print(predictions)\n",
    "Yg_pred = model.predict_classes(Xg_test)\n",
    "    \n",
    "f1 = f1_score(Yg_test, Yg_pred, average='weighted')\n",
    "print(\"f1 score: {}\".format(f1))\n",
    "f1_scores_learning.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9517932604948441, 0.9577138280605294, 0.963626117614296]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 : Init (base)\n",
    "Nombre de perceptrons dans la\n",
    "couche cachée (hidden layer)\n",
    "100, 100, 2\n",
    "Nombre d’itérations (epochs) 60\n",
    "Taux d’apprentissage (learning\n",
    "rate)\n",
    "0.0005\n",
    "Batch size 100\n",
    "\n",
    "## Results : loss: 0.2496 - acc: 0.9063 - val_loss: 0.1959 - val_acc: 0.9370\n",
    "\n",
    "\n",
    "\n",
    "# 2\n",
    "## Parameters\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 50\n",
    "\n",
    "## Network Parameters\n",
    "n_hidden_1 = 60 # 1st layer number of neurons\n",
    "n_hidden_2 = 30 # 2nd layer number of neurons\n",
    "num_classes = 2\n",
    "## Results : loss: 0.3018 - acc: 0.8822 - val_loss: 0.2372 - val_acc: 0.9178\n",
    "\n",
    "\n",
    "\n",
    "# Faire varier les paramètres : (12 modèles différents)\n",
    "\n",
    "## Le nombre d’itérations (epochs)\n",
    "### 100\n",
    "loss: 0.2116 - acc: 0.9204 - val_loss: 0.1544 - val_acc: 0.9503\n",
    "\n",
    "### 200\n",
    "loss: 0.1927 - acc: 0.9283 - val_loss: 0.1426 - val_acc: 0.9548\n",
    "\n",
    "### 400 LOL\n",
    "loss: 0.1391 - acc: 0.9472 - val_loss: 0.1100 - val_acc: 0.9622\n",
    "\n",
    "### 600\n",
    "loss: 0.1231 - acc: 0.9547 - val_loss: 0.1086 - val_acc: 0.9651\n",
    "\n",
    "\n",
    "## Le nombre de perceptrons dans ces couches intermédiaires\n",
    "\n",
    "### 100, 50\n",
    "loss: 0.2153 - acc: 0.9166 - val_loss: 0.1691 - val_acc: 0.9385\n",
    "\n",
    "### 50, 100\n",
    "loss: 0.2213 - acc: 0.9176 - val_loss: 0.1586 - val_acc: 0.9447\n",
    "\n",
    "### 200, 25\n",
    "loss: 0.2022 - acc: 0.9233 - val_loss: 0.1646 - val_acc: 0.9450\n",
    "\n",
    "### 50, 25\n",
    "loss: 0.2784 - acc: 0.8918 - val_loss: 0.2254 - val_acc: 0.9199\n",
    "\n",
    "## Le nombre de couches\n",
    "### 3 couches cachées\n",
    "\n",
    "loss: 0.2121 - acc: 0.9194 - val_loss: 0.1641 - val_acc: 0.9420\n",
    "#### dropout 0.5 pour chaque couche\n",
    "loss: 0.3504 - acc: 0.8592 - val_loss: 0.2258 - val_acc: 0.9222\n",
    "#### dropout 0.3 pour chaque couche\n",
    "loss: 0.2661 - acc: 0.8988 - val_loss: 0.1846 - val_acc: 0.9320\n",
    "\n",
    "### 4 couches cachées\n",
    "loss: 0.2393 - acc: 0.9094 - val_loss: 0.1699 - val_acc: 0.9420\n",
    "\n",
    "### 5 couches cachées\n",
    "loss: 0.2579 - acc: 0.8981 - val_loss: 0.1821 - val_acc: 0.9323\n",
    "\n",
    "## Le taux d’apprentissage (learning rate)\n",
    "### 3\n",
    "loss: 0.1061 - acc: 0.9607 - val_loss: 0.0942 - val_acc: 0.9663  \n",
    "#### bis\n",
    "loss: 0.0913 - acc: 0.9636 - val_loss: 0.1049 - val_acc: 0.9633\n",
    "\n",
    "### 0.1\n",
    "loss: 0.0961 - acc: 0.9649 - val_loss: 0.0869 - val_acc: 0.9687\n",
    "\n",
    "### 0.000001\n",
    "loss: 1.2686 - acc: 0.4354 - val_loss: 1.1457 - val_acc: 0.4243\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sans dropout et avec F1 score\n",
    "\n",
    "## Le nombre d’itérations (epochs)\n",
    "### 100\n",
    "loss: 0.1384 - acc: 0.9473 - val_loss: 0.1387 - val_acc: 0.9518\n",
    "Test accuracy: 0.9518036663638002\n",
    "f1 score : 0.9517932604948441\n",
    "\n",
    "### 200\n",
    "loss: 0.1134 - acc: 0.9590 - val_loss: 0.1213 - val_acc: 0.9577\n",
    "Test accuracy: 0.9577173269196843\n",
    "f1 score :0.9577138280605294\n",
    "\n",
    "### 400 \n",
    "loss: 0.0955 - acc: 0.9650 - val_loss: 0.1093 - val_acc: 0.9636\n",
    "Test accuracy: 0.9636309874755683\n",
    "f1 score :0.963626117614296\n",
    "\n",
    "### 600\n",
    "\n",
    "## Le nombre de perceptrons dans ces couches intermédiaires\n",
    "\n",
    "### 100, 50\n",
    "loss: 0.1675 - acc: 0.9382 - val_loss: 0.1640 - val_acc: 0.9527  \n",
    "Test accuracy: 0.9526907155529273  \n",
    "f1 score: 0.9526907155529273\n",
    "\n",
    "### 50, 100\n",
    "loss: 0.1722 - acc: 0.9372 - val_loss: 0.1792 - val_acc: 0.9409  \n",
    "Test accuracy: 0.9408633943354145  \n",
    "f1 score: 0.9408263855468233\n",
    "\n",
    "### 200, 25\n",
    "loss: 0.1579 - acc: 0.9422 - val_loss: 0.1629 - val_acc: 0.9438  \n",
    "Test accuracy: 0.9438202247191011  \n",
    "f1 score: 0.9438398613167942\n",
    "\n",
    "### 50, 25\n",
    "loss: 0.1777 - acc: 0.9333 - val_loss: 0.1861 - val_acc: 0.9400  \n",
    "Test accuracy: 0.939976345252032  \n",
    "f1 score: 0.9399831073373335\n",
    "\n",
    "## Le nombre de couches\n",
    "### 3 couches cachées\n",
    "loss: 0.1513 - acc: 0.9424 - val_loss: 0.1448 - val_acc: 0.9503  \n",
    "Test accuracy: 0.9503252512248291  \n",
    "f1 score: 0.9503152269164241\n",
    "\n",
    "### 4 couches cachées\n",
    "loss: 0.1437 - acc: 0.9451 - val_loss: 0.1497 - val_acc: 0.9471  \n",
    "Test accuracy: 0.9470727379190929  \n",
    "f1 score: 0.9470733265279817\n",
    "\n",
    "\n",
    "### 5 couches cachées\n",
    "loss: 0.1470 - acc: 0.9426 - val_loss: 0.1617 - val_acc: 0.9432  \n",
    "Test accuracy: 0.9432288586635127  \n",
    "f1 score: 0.9432071322488779\n",
    "\n",
    "## Le taux d’apprentissage (learning rate)\n",
    "### 3\n",
    "loss: 0.0661 - acc: 0.9752 - val_loss: 0.3223 - val_acc: 0.9119  \n",
    "Test accuracy: 0.9118864576115825  \n",
    "f1 score: 0.9116155733993972\n",
    "\n",
    "### 0.1\n",
    "loss: 0.0232 - acc: 0.9905 - val_loss: 0.1833 - val_acc: 0.9539  \n",
    "Test accuracy: 0.9538734475583596  \n",
    "f1 score: 0.9538392862815737\n",
    "\n",
    "### 0.000001\n",
    "loss: 0.9533 - acc: 0.5129 - val_loss: 0.9114 - val_acc: 0.5269  \n",
    "Test accuracy: 0.5269071556702652  \n",
    "f1 score: 0.5270565773067073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(Xg_train, Yg_train, epochs=n_epochs, batch_size=batch, validation_data=(Xg_test, Yg_test))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = n_epochs\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Smooth/Spiral\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"search_overfit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(3)\n",
    "plt.bar(x, height= accuracies, )\n",
    "#plt.xticks(x+.5, ['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gti770_env]",
   "language": "python",
   "name": "conda-env-gti770_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
