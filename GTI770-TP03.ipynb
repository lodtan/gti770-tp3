{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 3 : Machines à vecteurs de support et réseaux neuronaux\n",
    "#### Département du génie logiciel et des technologies de l’information\n",
    "\n",
    "| Étudiants             | LEMARCHANT HUGO - LEMH03039705 * TAN ELODIE - TANE25619607 * JACQUES-SYLVAIN LECOINTRE LECJ19128301|\n",
    "|-----------------------|---------------------------------------------------------|\n",
    "| Cours                 | GTI770 - Systèmes intelligents et apprentissage machine |\n",
    "| Session               | Automne 2018                                            |\n",
    "| Groupe                | C                                                       |\n",
    "| Numéro du laboratoire | 02                                                      |\n",
    "| Professeur            | Prof. Hervé Lombaert                                    |\n",
    "| Chargé de laboratoire | Pierre-Luc Delisle                                      |\n",
    "| Date                  | 31/10/2018                                              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score , GridSearchCV , StratifiedShuffleSplit\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vectors = pd.read_csv('galaxy_feature_vectors.csv', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_galaxy = pd.read_csv('galaxy_feature_vectors.csv', delimiter=',', header=None).values[:, 0:-1]\n",
    "Y_galaxy = pd.read_csv('galaxy_feature_vectors.csv', delimiter=',', header=None).values[:, -1:].astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_galaxy, Y_galaxy, test_size=0.20, random_state=42,stratify=Y_galaxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machines à vecteurs de support "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs = 6\n",
    "cache_size=2048\n",
    "k=5\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=k, test_size=0.2, random_state=42)\n",
    "svc = SVC(cache_size=cache_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recherche par grille : Lineaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid_linear = {'kernel': ['linear'], 'C': [10 ** (-3), 10 ** (-1), 1, 10], 'class_weight': ['balanced'],'gamma':['scale']}\n",
    "grid_linear = GridSearchCV(svc, param_grid=param_grid_linear, cv=cv, n_jobs=jobs, scoring='accuracy',verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "grid_linear.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"LINEAR : The best hyperparameters are %s with a score of %0.2f\"% (grid_linear.best_params_, grid_linear.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recherche par grille : RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid_rbf = {'kernel':('rbf'),'C':[10**(-3), 10**(-1), 1, 10],'gamma':[10**(-3), 10**(-1), 1, 10]}\n",
    "grid_rbf = GridSearchCV(svc, param_grid=param_grid_rbf, cv=cv, n_jobs=jobs, scoring='accuracy',verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_rbf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"RBF : The best hyperparameters are %s with a score of %0.2f\" % (grid_rbf.best_params_, grid_rbf.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_range=param_grid_rbf['C']\n",
    "gamma_range=param_grid_rbf['gamma']\n",
    "\n",
    "grid=grid_rbf\n",
    "score_dict = grid.grid_scores_\n",
    "\n",
    "# We extract just the scores\n",
    "scores = [x[1] for x in score_dict]\n",
    "scores = np.array(scores).reshape(len(C_range), len(gamma_range))\n",
    "\n",
    "# Make a nice figure\n",
    "pl.figure(figsize=(8, 6))\n",
    "pl.subplots_adjust(left=0.15, right=0.95, bottom=0.15, top=0.95)\n",
    "pl.imshow(scores, interpolation='nearest', cmap=pl.cm.spectral)\n",
    "pl.xlabel('gamma')\n",
    "pl.ylabel('C')\n",
    "pl.colorbar()\n",
    "pl.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "pl.yticks(np.arange(len(C_range)), C_range)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "matplotlib.use(\"Agg\")\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.models import load_model\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "class NetCNN:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        # initialize the model\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    " \n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "        # first set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(20, (5, 5), padding=\"same\",input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # second set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(50, (5, 5), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # third set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(20, (5, 5), padding=\"same\",input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.3))\n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model\n",
    "    \n",
    "# initialize the data and labels\n",
    "print(\"[INFO] loading images...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "with open(\"..\\\\data\\\\data\\\\csv\\\\galaxy\\\\galaxy_label_data_set.csv\", 'rt') as csvFile:\n",
    "    reader = csv.reader(csvFile, delimiter=\",\")\n",
    "    # loop over the input images\n",
    "    for index, row in enumerate(reader):\n",
    "        if index in range(1,1000):\n",
    "            # load the image, pre-process it, and store it in the data list\n",
    "            file = \"..\\\\data\\\\data\\\\images\\\\\"+row[0]+\".jpg\"\n",
    "            image = cv2.imread(file)\n",
    "            image = cv2.resize(image, (212, 212))\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "            label = (0 if row[1] == 'smooth' else 1)\n",
    "            labels.append(label)\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "trainY = to_categorical(trainY, num_classes=2)\n",
    "testY = to_categorical(testY, num_classes=2)\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,height_shift_range=0.1, \n",
    "      shear_range=0.2, zoom_range=0.2,horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "EPOCHS = 50\n",
    "INIT_LR = 1e-3\n",
    "BS = 25\n",
    "\n",
    "# initialize the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "model = NetCNN.build(width=212, height=212, depth=3, classes=2)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=EPOCHS, verbose=1)\n",
    "\n",
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(\"..\\\\data\\\\tp3\\\\model.hdf5\")\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = EPOCHS\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Smooth/Spiral\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"Graphs\\\\3c25bs3lr.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "### Présentation de la méthode de validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Description du modèle élaboré"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Analyse de la phase d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### Matrice des expérimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Étude des hyperparamètres des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "### Matrice des expérimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Études des hyperparamètres des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "### Discussion de l'impact de la taille des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "### Discussion de l'impact du bruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "### Formulation des recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "### Améliorations possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Bonus\n",
    "\n",
    "Le réseau de neurones convolutif est un réseau de neurones classique à la différence que les neurones de la couche de convolution partagent le même poids, qui est en fait une fenêtre de convolution. C’est à dire dans notre cas une matrice 5x5 de poids. De cette manière chaque neurone correspond à un déplacement de la matrice sur l’image (un déplacement de la fenêtre de convolution). L’objectif pour ces couches est de trouver les poids de cette matrice tels qu’ils minimisent la fonction de coût (ici binary_crossentropy). Ensuite après cette couche de convolution, nous faisons un maxPooling qui revient à considérer un cadre de 2x2 neurones et de synthétiser cette fenêtre en la valeur maximale présente dans cette fenêtre (on utilise le max plutôt que la moyenne car la littérature montre que le maximum décrit mieux la fenêtre que la moyenne).\n",
    "\n",
    "Nos de convolutions suivies de maxPooling activées avec ReLU, une couche dense (ou fully-connected) avec la même fonction d’activation ReLU et enfin une couche de décision de deux neurones avec une activation softMax pour traduire les résultats en probabilités.\n",
    "\n",
    "Nous avons de plus ajouté un dropout sur nos différentes couches (20% et 30% pour les couches de convolution et la couche dense) pour empêcher notre réseau de faire du sur-apprentissage. En effet avec ces extinction de neurones, on entraîne virtuellement un forêt de réseaux tels que le réseau complet une fois entraîné sera la décision moyenne de cette forêt.\n",
    "Pour ce classifieur, nous avions accès à trois hyper-paramètres : le taux d’apprentissage, la taille du mini batch et le nombre de couches de convolutions.\n",
    "\n",
    "Pour la taille du mini batch, nous ne pouvons pas faire varier énormément car si celle ci devient trop importante le batch ne peut plus être chargé en mémoire. Nous avons donc testé avec des tailles de 25 et 10. Le taux d'apprentissage initial a été paramétré à 1e-1, 1e-3 et 1e-5. Nous avons comparé d’un autre côté l’apprentissage via 2 et 3 couches de convolution.\n",
    "\n",
    "Ci dessous la comparaison entre les deux tailles de mini batch:\n",
    "\n",
    "__Batch size = 10__\n",
    "![\"Batch size = 10\"](Graphs/2c10bs3lr.png)\n",
    "__Batch size = 25__\n",
    "![\"Batch size = 25\"](Graphs/2c25bs3lr.png)\n",
    "\n",
    "Ci dessous la comparaison entre deux et trois couches convolutives:\n",
    "\n",
    "__3 couches de convolution__\n",
    "![\"3 couches de convolution\"](Graphs/3c25bs3lr.png)\n",
    "__2 couches de convolution__\n",
    "![\"2 couches de convolution\"](Graphs/2c25bs3lr.png)\n",
    "\n",
    "Ci dessous la comparaison entre 3 valeurs de learning rate:\n",
    "\n",
    "__Learning rate = 1e-1__\n",
    "![\"Learning rate = 1e-1\"](Graphs/2c25bs1lr.png)\n",
    "__Learning rate = 1e-5__\n",
    "![\"Learning rate = 1e-5\"](Graphs/2c25bs5lr.png)\n",
    "__Learning rate = 1e-3__\n",
    "![\"Learning rate = 1e-3\"](Graphs/2c25bs3lr.png)\n",
    "\n",
    "On observe que le batch size de 10 permet un apprentissage plus constant, les écarts entre validation et entraînement sont moins importants. On observe aussi des meilleurs résultats sur les 10 premières itérations. Les mêmes effets sont visibles à l’ajout d’une troisième couche de convolution.\n",
    "Le learning rate lui à des effets bien plus visibles, un learning rate très grand fait diverger instantanément le réseau et le gradient devient très grand. Le learning rate de 1e-5 lui donne une courbe très lisse avec une précision égale au terme des 50 itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliographie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
