{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 3 : Machines à vecteurs de support et réseaux neuronaux\n",
    "#### Département du génie logiciel et des technologies de l’information\n",
    "\n",
    "| Étudiants             | LEMARCHANT HUGO - LEMH03039705 * TAN ELODIE - TANE25619607 * JACQUES-SYLVAIN LECOINTRE LECJ19128301|\n",
    "|-----------------------|---------------------------------------------------------|\n",
    "| Cours                 | GTI770 - Systèmes intelligents et apprentissage machine |\n",
    "| Session               | Automne 2018                                            |\n",
    "| Groupe                | C                                                       |\n",
    "| Numéro du laboratoire | 02                                                      |\n",
    "| Professeur            | Prof. Hervé Lombaert                                    |\n",
    "| Chargé de laboratoire | Pierre-Luc Delisle                                      |\n",
    "| Date                  | 31/10/2018                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use(\"Agg\")\n",
    "from time import time\n",
    "from IPython.display import Markdown\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = pd.read_csv('galaxy_feature_vectors.csv', delimiter = ',', header=None)\n",
    "labels = pd.read_csv('galaxy_label_data_set.csv', delimiter = ',')\n",
    "X_galaxy = pd.read_csv('galaxy_feature_vectors.csv', delimiter = ',', header=None).values[:,0:-1]\n",
    "Y_galaxy = pd.read_csv('galaxy_feature_vectors.csv', delimiter = ',', header=None).values[:,-1:].astype(int).flatten()\n",
    "Xg_train, Xg_test, Yg_train, Yg_test = train_test_split(X_galaxy, Y_galaxy, test_size=0.20, random_state=42, stratify=Y_galaxy)\n",
    "\n",
    "# On normalise nos features\n",
    "scaler = StandardScaler()\n",
    "Xg_train = scaler.fit_transform(Xg_train)\n",
    "Xg_test = scaler.fit_transform(Xg_test)\n",
    "# Création d'array pour stocker l'accuracy et le score f1 pour les différents nombres d'itération\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "accuracies_npercep = []\n",
    "f1_scores_npercep = []\n",
    "accuracies_layers = []\n",
    "f1_scores_layers = []\n",
    "accuracies_learning = []\n",
    "f1_scores_learning = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layers Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 100\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 60\n",
    "n_hidden_1 = 100 # 1st layer number of neurons\n",
    "n_hidden_2 = 100 # 2nd layer number of neurons\n",
    "n_hidden_3 = 50 # 3rd layer number of neurons\n",
    "n_hidden_4 = 2 # 4th layer number of neurons\n",
    "num_input = 75\n",
    "num_classes = 2\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(n_hidden_1))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(n_hidden_2))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='no_dropout/learning_1E-3', batch_size=batch, histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "model.fit(Xg_train, Yg_train, epochs=n_epochs, batch_size=batch, validation_data=(Xg_test, Yg_test),\n",
    "          callbacks=[tensorboard])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(Xg_test, Yg_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "predictions = model.predict(Xg_test)\n",
    "print(predictions)\n",
    "Yg_pred = model.predict_classes(Xg_test)\n",
    "    \n",
    "f1 = f1_score(Yg_test, Yg_pred, average='weighted')\n",
    "print(\"f1 score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "\n",
    "class NetCNN:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        # initialize the model\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    " \n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "        # first set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(20, (5, 5), padding=\"same\",input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # second set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(50, (5, 5), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # third set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(20, (5, 5), padding=\"same\",input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.3))\n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model\n",
    "    \n",
    "# initialize the data and labels\n",
    "print(\"[INFO] loading images...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "with open(\"..\\\\data\\\\data\\\\csv\\\\galaxy\\\\galaxy_label_data_set.csv\", 'rt') as csvFile:\n",
    "    reader = csv.reader(csvFile, delimiter=\",\")\n",
    "    # loop over the input images\n",
    "    for index, row in enumerate(reader):\n",
    "        if index in range(1,1000):\n",
    "            # load the image, pre-process it, and store it in the data list\n",
    "            file = \"..\\\\data\\\\data\\\\images\\\\\"+row[0]+\".jpg\"\n",
    "            image = cv2.imread(file)\n",
    "            image = cv2.resize(image, (212, 212))\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "            label = (0 if row[1] == 'smooth' else 1)\n",
    "            labels.append(label)\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "trainY = to_categorical(trainY, num_classes=2)\n",
    "testY = to_categorical(testY, num_classes=2)\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,height_shift_range=0.1, \n",
    "      shear_range=0.2, zoom_range=0.2,horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "EPOCHS = 50\n",
    "INIT_LR = 1e-3\n",
    "BS = 25\n",
    "\n",
    "# initialize the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "model = NetCNN.build(width=212, height=212, depth=3, classes=2)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=EPOCHS, verbose=1)\n",
    "\n",
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(\"..\\\\data\\\\tp3\\\\model.hdf5\")\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = EPOCHS\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Smooth/Spiral\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"Graphs\\\\3c25bs3lr.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Etant donné que les questions du notebook et celles de l'énnocé n'étaient pas dans le même ordre, nous avons choisi de répondre à l'ordre proposé par l'énnoncé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "### Présentation de la méthode de validation\n",
    "\n",
    "Nous avons opté pour une validation par hold-out. En effet toutes les autres méthodes sont gourmandes voir extrèmement gourmandes en temps de calcul pour les réseaux de neurones ou les SVM. Nous nous sommes donc contentés de séparer nos données en deux ensembles d'entraînement et de validation.\n",
    "Pour la SVM, nous avons procédé à une recherche par grille des meilleurs hyper-paramètres sur une fraction de notre enssemble d'entraînement et chaque couple d'hyper-paramètres est validé par 3-fold-CV. Une fois ces paramètres optimisés nous avons entraîné le modèle avec l'ensemble d'entraînement privé des données de validations puis avons fais les tests de validation.\n",
    "\n",
    "De manière équivalente pour les réseaux de neurones (et CNN) nous avons découpé notre ensemble en deux et avons fait nos tests d'hyper-paramètres avec ces ensemble d'entraînement et nous les comparons entre eux avec l'ensemble de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Description de la méthode de normalisation\n",
    "\n",
    "#### Réseaux de neurones\n",
    "\n",
    "Nous avons utilisé pour les réseaux de neurones la normalisation avec `StandardScaler()` de la librairie `scikitlearn`. Elle permet de transformer nos données en faisant en sorte d’avoir une moyenne de 0 et un écart type de 1, ce qui permet de d’éliminer en quelque sorte les outliers mais aussi de mettre à la même échelle l’ensemble des variables. Nous avons aussi essayé d’utiliser la fonction `normalize` de la librairie `sklearn.preprocessing`. Cependant, son utilisation n’était pas pertinente et les résultats étaient erronés car elle agit sur toute une ligne (et non pas une colonne comme `StandardScaler`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Description du modèle élaboré et analyse de la phase d'entrainement\n",
    "\n",
    "#### Structure et choix du modèle d’apprentissage\n",
    "Nous avons créé deux couches cachées avec la fonction d’activation **ReLU**. Elle est beaucoup plus efficace que la fonction $tanh$, et c’est une fonction simple à dériver. De même, la fonction $Sigmoid$ prend beaucoup de temps pour converger et d’autres problèmes existent pour cette fonction comme le problème du *vanishing gradient*.  Nous n’utilisons pas $Heaviside$ car celle-ci n’est pas dérivable.\n",
    "\n",
    "De plus, nous avons choisi d’effectuer une descente de gradient par mini-batch. Il s’agit d’un compromis entre la descente stochastique de gradient qui calcule l’erreur pour chaque échantillon et l’apprentissage par batch qui calcule l’erreur sur l’ensemble de la base de données. La variation de la taille des lots (batchs) permet de faire varier ce compromis. Cependant nous ne faisons pas varier le nombre de batch dans ce laboratoire et l’avons laissé à 100.\n",
    "\n",
    "Après chaque couche cachée, nous avons utilisé `BatchNormalisation()` et `Dropout()`. `BatchNormalisation()` permet au réseau de neurones d’apprendre plus rapidement (mais pas forcément mieux) en normalisant chaque $batch$ comme avec `StandardScaler()`. En effet, lorsque nous ajustons les poids, nos données changent et ne sont plus normalisées.  \n",
    "\n",
    "`Dropout()` permet de désactiver de façon aléatoire certains neurones d’une couche. Cela permet d’éviter le sur-apprentissage. Nous avons donc mis, pour chaque couche cachée, un taux de `dropout` de 0.2. Nous n’avons pas pris de valeurs plus élevée car cela impacte aussi le taux de $accuracy$.   \n",
    "\n",
    "Pour finir, nous appliquons pour la dernière couche de neurones la fonction d’activation `softmax` qui permet d’obtenir en sortie les probabilités correspondant à chaque classe. \n",
    "\n",
    "#### Fonction de coût\n",
    "Nous avons utilisé `sparse_categorical_crossentropy` pour la fonction de coût. Tout d’abord, `crossentropy` correspond bien évidemment à la $Cross-Entropy$. Elle permet d’être plus rapide contrairement à la fonction d’erreur quadratique car celle-ci est très coûteuse en temps de calcul pour une machine. De plus, `sparse_categorical_crossentropy` permet l’utilisation de la fonction sur des labels qui sont donnés dans une seule colonne (c’est à dire le numéro du label reçu), contrairement à `categorical_crossentropy` qui nous obligeait à les coder en `one-hot`. Nous n’avons pas utilisé `binary_crossentropy` pour la même raison. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### Etude de l'évolution temporelle de l'apprentissage\n",
    "\n",
    "L’overfitting arrive généralement lorsque l’ensemble de validation rencontre un minimum local à travers les itérations pour le coût. Dans nos graphiques, cette condition arrive lorsque nous sommes à 20 itérations pour un learning rate de 0.1. Cette valeur change en fonction du learning rate choisi. Cependant, parmi nos trois modèles (3, 0.1 et 1e-6), le learning rate qui donnait le meilleur résultat était de 0.1, ce qui semble évident puisqu’un learning rate élevé est moins stable, et un learning rate trop faible prend beaucoup plus de temps à converger. Le nombre optimal pour notre modèle semble donc être 20. En effet, après 20, le gain d’accuracy pour l’ensemble de validation est assez faible, et la diminution du coût aussi. Nous obtenons un bon rapport qualité/temps avec 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "### Matrice des expérimentations\n",
    "\n",
    "#### Impact du nombre d’epochs\n",
    "\n",
    "La généralisation du modèle est ce que l’on cherche à obtenir. Le modèle doit être capable de généraliser et donc de prédire des  nouvelles données. Pour éviter le surajustement il faut donc être capable de déterminer à quel moment arrêter l'entraînement pour minimiser les erreurs d’apprentissages tout en minimisant les erreurs de tests. Pour déterminer le nombre de cycles d’apprentissage (epochs), nous utilisons la base de validation.Cet hyperparamètre a un impact sur le sur-apprentissage.\n",
    " \n",
    "Cette interruption prématurée (early-stop) est mise en œuvre en présentant les échantillons de la base de validation à chaque cycle d'entraînement et en utilisant la fonction d’erreur pour calculer l’erreur de ce cycle. Lorsque l’erreur de la base de validation ne diminue plus ou se met à augmenter, l’apprentissage est alors interrompu.\n",
    "\n",
    "![\"NbEpochs\"](Graphs/image.png)\n",
    "\n",
    "#### Impact du nombre de couches\n",
    "Sans couche cachée,les données doivent être séparables linéairement. l’ajout de couches permet de projeter les données dans un espace où les données deviennent linéairement séparables.\n",
    "\n",
    "Avec une deux couches  cachées, les frontières de décision peuvent théoriquement être d’une complexité arbitraire.\n",
    "\n",
    "![\"NbLayers\"](Graphs/image1.png)\n",
    "\n",
    "#### Impact du nombre de perceptrons dans les couches intermédiaires\n",
    "\n",
    "Le nombre de perceptrons dans les couches d'entrée et de sortie sont fixés et dépendent respectivement du nombre de dimensions (taille des vecteurs de caractéristiques) et du nombre de classes (2). Un perceptron implémente un hyperplan, Augmenter le nombre de perceptrons permet donc d’obtenir des régions de décision plus complexes constituées de plusieurs hyperplans.\n",
    "\n",
    "![\"NbPerceptrons\"](Graphs/image3.png)\n",
    "\n",
    "#### Impact du taux d’apprentissage (learning rate)\n",
    "\n",
    "Le taux d’apprentissage permet de contrôler la vitesse avec laquelle les poids sont ajustés lors de la descente de gradient.La retro-propagation de l’erreur permet de converger vers un minimum local de la fonction de coût.Si le taux d’apprentissage est trop élevé,la descente de gradient pourrait ne pas converger .Inversement ,un taux d’apprentissage trop faible ralenti la vitesse de convergence de la descente de gradient et donc être un obstacle pour des réseaux profonds qui nécessitent beaucoup de calcul.\n",
    "\n",
    "![\"LearningRate\"](Graphs/image2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec drop-out : Loss et Accuracies\n",
      "                            loss     acc  val_loss  val_acc\n",
      "avec paramètres initiaux  0.2496  0.9063    0.1959   0.9370\n",
      "100 epochs                0.2116  0.9204    0.1544   0.9503\n",
      "200 epochs                0.1927  0.9283    0.1426   0.9548\n",
      "400 epochs                0.1391  0.9472    0.1100   0.9622\n",
      "600 epochs                0.1231  0.9547    0.1086   0.9651\n",
      "                       loss     acc  val_loss  val_acc\n",
      "100, 50 perceptrons  0.2153  0.9166    0.1691   0.9385\n",
      "50, 100              0.2213  0.9176    0.1586   0.9447\n",
      "200, 25              0.2022  0.9233    0.1646   0.9450\n",
      "50, 25               0.2784  0.8918    0.2254   0.9199\n",
      "                       loss     acc  val_loss  val_acc\n",
      "100, 50 perceptrons  0.2153  0.9166    0.1691   0.9385\n",
      "50, 100              0.2213  0.9176    0.1586   0.9447\n",
      "200, 25              0.2022  0.9233    0.1646   0.9450\n",
      "50, 25               0.2784  0.8918    0.2254   0.9199\n",
      "                       loss     acc  val_loss  val_acc\n",
      "learning rate: 3     0.1061  0.9607    0.0942   0.9663\n",
      "learning rate: 0.1   0.0961  0.9649    0.0869   0.9687\n",
      "learning rate: 1E-6  1.2686  0.4354    1.1457   0.4243\n",
      "\n",
      "\n",
      "Sans drop-out : Loss, Accuracies avec F1-mesures\n",
      "                            loss     acc  val_loss  val_acc  test_accuracy  \\\n",
      "avec paramètres initiaux  0.1577  0.9415    0.1729   0.9468       0.946777   \n",
      "100 epochs                0.1384  0.9473    0.1387   0.9500       0.951804   \n",
      "200 epochs                0.1134  0.9590    0.1213   0.9500       0.957717   \n",
      "400 epochs                0.0955  0.9650    0.1093   0.9600       0.963631   \n",
      "\n",
      "                          F1 score  \n",
      "avec paramètres initiaux  0.946785  \n",
      "100 epochs                0.951793  \n",
      "200 epochs                0.957714  \n",
      "400 epochs                0.963626  \n",
      "                       loss     acc  val_loss  val_acc  test_accuracy  \\\n",
      "100, 50 perceptrons  0.1675  0.9382    0.1640   0.9527       0.952691   \n",
      "50, 100              0.1722  0.9372    0.1792   0.9409       0.940863   \n",
      "200, 25              0.1579  0.9422    0.1629   0.9438       0.943820   \n",
      "50, 25               0.1777  0.9333    0.1861   0.9400       0.939976   \n",
      "\n",
      "                     F1 score  \n",
      "100, 50 perceptrons  0.952691  \n",
      "50, 100              0.940826  \n",
      "200, 25              0.943840  \n",
      "50, 25               0.939983  \n",
      "                       loss     acc  val_loss  val_acc  test_accuracy  \\\n",
      "100, 50 perceptrons  0.1675  0.9382    0.1640   0.9527       0.952691   \n",
      "50, 100              0.1722  0.9372    0.1792   0.9409       0.940863   \n",
      "200, 25              0.1579  0.9422    0.1629   0.9438       0.943820   \n",
      "50, 25               0.1777  0.9333    0.1861   0.9400       0.939976   \n",
      "\n",
      "                     F1 score  \n",
      "100, 50 perceptrons  0.952691  \n",
      "50, 100              0.940826  \n",
      "200, 25              0.943840  \n",
      "50, 25               0.939983  \n",
      "                       loss     acc  val_loss  val_acc  test_accuracy  \\\n",
      "learning rate: 3     0.0661  0.9752    0.3223   0.9119       0.911886   \n",
      "learning rate: 0.1   0.0232  0.9905    0.1833   0.9539       0.953873   \n",
      "learning rate: 1E-6  0.9533  0.5129    0.9114   0.5269       0.526907   \n",
      "\n",
      "                     F1 score  \n",
      "learning rate: 3     0.911616  \n",
      "learning rate: 0.1   0.953839  \n",
      "learning rate: 1E-6  0.527057  \n"
     ]
    }
   ],
   "source": [
    "print(\"Avec drop-out : Loss et Accuracies\")\n",
    "\n",
    "ar = np.array([[0.2496, 0.9063, 0.1959, 0.9370], \n",
    "                  [0.2116, 0.9204, 0.1544, 0.9503], \n",
    "                  [0.1927, 0.9283, 0.1426, 0.9548], \n",
    "                  [0.1391, 0.9472, 0.1100, 0.9622],\n",
    "                  [0.1231, 0.9547, 0.1086, 0.9651]])\n",
    "dfepoch = pd.DataFrame(ar, \n",
    "                      index = ['avec paramètres initiaux','100 epochs', '200 epochs', '400 epochs', '600 epochs'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "print(dfepoch)\n",
    "\n",
    "ar = np.array([[0.2153, 0.9166, 0.1691, 0.9385], \n",
    "                  [0.2213, 0.9176, 0.1586, 0.9447],\n",
    "                  [0.2022, 0.9233, 0.1646, 0.9450],\n",
    "                  [0.2784, 0.8918, 0.2254, 0.9199]])\n",
    "dfpercep = pd.DataFrame(ar, \n",
    "                      index = ['100, 50 perceptrons', '50, 100', '200, 25', '50, 25'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "print(dfpercep)\n",
    "\n",
    "ar = np.array([[0.2121, 0.9194, 0.1641, 0.9420], \n",
    "                  [0.2393, 0.9094, 0.1699, 0.9420], \n",
    "                  [0.2579, 0.8981, 0.1821, 0.9323]])\n",
    "dflayers = pd.DataFrame(ar, \n",
    "                      index = ['3 couches cachées', '4 couches', '5 couches'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "print(dfpercep)\n",
    "\n",
    "ar = np.array([[0.1061, 0.9607, 0.0942, 0.9663], \n",
    "                  [0.0961, 0.9649, 0.0869, 0.9687], \n",
    "                  [1.2686, 0.4354, 1.1457, 0.4243]])\n",
    "dflearning = pd.DataFrame(ar, \n",
    "                      index = ['learning rate: 3', 'learning rate: 0.1', 'learning rate: 1E-6'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "print(dflearning)\n",
    "\n",
    "print(\"\\n\\nSans drop-out : Loss, Accuracies avec F1-mesures\")\n",
    "\n",
    "ar = np.array([[0.1577, 0.9415, 0.1729, 0.9468, 0.9467770548912987, 0.9467854026859117], \n",
    "                  [0.1384, 0.9473, 0.1387, 0.95, 0.9518036663638002, 0.9517932604948441\n",
    "], \n",
    "                  [0.1134, 0.9590, 0.1213, 0.95, 0.9577173269196843, 0.9577138280605294], \n",
    "                  [0.0955, 0.9650, 0.1093, 0.96, 0.9636309874755683, 0.963626117614296]])\n",
    "dfepoch = pd.DataFrame(ar, \n",
    "                      index = ['avec paramètres initiaux','100 epochs', '200 epochs', '400 epochs'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc', 'test_accuracy', 'F1 score'])\n",
    "\n",
    "print(dfepoch)\n",
    "\n",
    "ar = np.array([[0.1675, 0.9382, 0.1640, 0.9527, 0.9526907155529273, 0.9526907155529273], \n",
    "                  [0.1722, 0.9372, 0.1792, 0.9409, 0.9408633943354145, 0.9408263855468233],\n",
    "                  [0.1579, 0.9422, 0.1629, 0.9438, 0.9438202247191011, 0.9438398613167942],\n",
    "                  [0.1777, 0.9333, 0.1861, 0.9400, 0.939976345252032, 0.9399831073373335]])\n",
    "dfpercep = pd.DataFrame(ar, \n",
    "                      index = ['100, 50 perceptrons', '50, 100', '200, 25', '50, 25'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc', 'test_accuracy', 'F1 score'])\n",
    "print(dfpercep)\n",
    "\n",
    "ar = np.array([[0.1513, 0.9424, 0.1448, 0.9503, 0.9503252512248291, 0.9503152269164241], \n",
    "                  [0.1437, 0.9451, 0.1497, 0.9471, 0.9470727379190929, 0.9470733265279817], \n",
    "                  [0.1470, 0.9426, 0.1617, 0.9432, 0.9432288586635127, 0.9432071322488779]])\n",
    "dflayers = pd.DataFrame(ar, \n",
    "                      index = ['3 couches cachées', '4 couches', '5 couches'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc', 'test_accuracy', 'F1 score'])\n",
    "print(dfpercep)\n",
    "\n",
    "ar = np.array([[0.0661, 0.9752, 0.3223, 0.9119, 0.9118864576115825, 0.9116155733993972], \n",
    "                  [0.0232, 0.9905, 0.1833, 0.9539, 0.9538734475583596, 0.9538392862815737], \n",
    "                  [0.9533, 0.5129, 0.9114, 0.5269, 0.5269071556702652, 0.5270565773067073]])\n",
    "dflearning = pd.DataFrame(ar, \n",
    "                      index = ['learning rate: 3', 'learning rate: 0.1', 'learning rate: 1E-6'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc', 'test_accuracy', 'F1 score'])\n",
    "print(dflearning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "### Présentation de la méthode de recherche des meilleurs hyper paramètres de SVM\n",
    "\n",
    "#### Méthode utilisée\n",
    "\n",
    "Nous avons utilisé la méthode de recherche par grille ( `GridSearch` avec sklearn ) qui permet de tester chaque combinaison d’hyper paramètres définie et de sélectionner celle qui maximise un certain critère. Dans le cadre de ce TP, nous avons choisi le critère d’accuracy pour sélectionner les hyper-parametres C et Gamma.Cette recherche par grille s’effectue sur un base de données de validation (sous ensemble de la base de données  d'entraînement) dédiée à cette recherche d'hyper paramètres. De plus,la recherche par gille a été réalisée avec une validation croisée ce qui permet d’avoir une plus grande certitude quant au choix du modèle SVM.\n",
    "\n",
    "#### Résultats\n",
    "\n",
    "Au total, 16 combinaisons d’hyper paramètres C et Gamma ont été testées pour le SVM non linéaire ce qui donne 160 itérations avec une 10-fold validation croisée.\n",
    "\n",
    "C est le compromis entre la pertinence des classifications (sur les données d'entraînement) et la taille de la marge (généralisation aux données de test).Idéalement, SVM souhaite maximiser la marge tout en ayant des classifications parfaites.\n",
    "\n",
    "C=10 et Gamma 0.001 est la meilleure combinaison d'hyper paramètres obtenue pour un SVM non-linéaire avec une fonction noyau de type RBF.\n",
    "\n",
    "Pour un SVM linéaire,  le meilleur hyper-paramètre obtenu est pour C=10.\n",
    "\n",
    "La plus grande valeur de C=10 a donc été retenue , ce qui assure donc peu d’erreurs de classifications sur les données d’entrainements et cette non-tolérance semble bien se généraliser sur les données de test. \n",
    "\n",
    "#### Impact des hyperparamètres et utilité\n",
    "\n",
    "L’approche SVM (linéaire) utilisée dans ce TP consiste à maximiser la marge entre les hyperplans qui séparent les classes et les vecteurs de support de ces classes. Maximiser cette marge de l’hyperplan M = 2 / ||w||  revient à minimiser la fonction de coût L(w) = ||w||^2/ 2 (critère d’optimisation quadratique) avec la contrainte que chaque échantillon de du vecteur de test soit correctement classifié.\n",
    " \n",
    "Dans le cas où les échantillons sont non séparables, un hyper-paramètre C est introduit qui est le compromis de la marge poreuse (soft-margin) .La fonction de coût à minimiser devient  alors sous contrainte.La tolérance aux erreurs est plus ou moins accentuée avec le paramètre C. Plus C est grand plus la distance entre les observation erronées et la droite définie par les vecteurs de support sera grande.\n",
    " \n",
    "Dans l'approche SVM non-linéaire,les vecteurs d’entrée sont projetés dans un espace de plus grande dimension avec une transformation non linéaire à l’aide d’une fonction noyau de type RBF qui ont des propriétés permettant de simplifier le calcul (astuce du noyau).\n",
    " \n",
    "L‘apprentissage devient un problème d’optimisation quadratique sous contraintes (avec méthode Lagrangienne et  un multiplicateur Lagrangien  ) . \n",
    " \n",
    "Dan ce cas il y  a deux hyperparamètres C et Gamma.Gamma contrôle la variance des fonctions noyaux (inverse de sigma) Plus gamma augmente plus les rayons autour des de chaque points seront resserrés ce qui entraîne du sur-apprentissage.A l’inverse , un Gamma trop petit donnera une mauvaise généralisation et un sous-apprentissage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "### Discussion de l'impact de la taille des données\n",
    "\n",
    "Pour des primitives intéressantes, une taille plus importante de données est bien sûr bénéfique car cela nous permettra de mieux généraliser notre problème, cependant cela va rallonger le temps d'entraînement de notre algorithme.Pour les SVM , la complexité de l’algorithme est quadratique avec la taille des données : O(nombre de caractéristiques x nombre d’observation^2), augmenter la taille des données pourrait rendre l’algorithme trop coûteux en mémoire et en CPU. Pour ce faire les prédispositions suivantes ont été prises pour compte-tenu de la taille des données :\n",
    "- L'utilisation appropriée du cache (2GB)\n",
    "- Normalisation des données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "### Formulation des recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "### Améliorations possibles\n",
    "\n",
    "Effectuer une meilleure normalisation avant d'entraîner les classifieurs. Nous pourrions aussi faire une réduction de dimensionnalité pour le SVM puisque la complexité de l’algorithme dépend directement du nombre de features donc cela pourrait être une piste d’amélioration pour nos classificateurs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Bonus\n",
    "\n",
    "Le réseau de neurones convolutif est un réseau de neurones classique à la différence que les neurones de la couche de convolution partagent le même poids, qui est en fait une fenêtre de convolution. C’est à dire dans notre cas une matrice 5x5 de poids. De cette manière chaque neurone correspond à un déplacement de la matrice sur l’image (un déplacement de la fenêtre de convolution). L’objectif pour ces couches est de trouver les poids de cette matrice tels qu’ils minimisent la fonction de coût (ici binary_crossentropy). Ensuite après cette couche de convolution, nous faisons un maxPooling qui revient à considérer un cadre de 2x2 neurones et de synthétiser cette fenêtre en la valeur maximale présente dans cette fenêtre (on utilise le max plutôt que la moyenne car la littérature montre que le maximum décrit mieux la fenêtre que la moyenne).\n",
    "\n",
    "Nos de convolutions suivies de maxPooling activées avec ReLU, une couche dense (ou fully-connected) avec la même fonction d’activation ReLU et enfin une couche de décision de deux neurones avec une activation softMax pour traduire les résultats en probabilités.\n",
    "\n",
    "Nous avons de plus ajouté un dropout sur nos différentes couches (20% et 30% pour les couches de convolution et la couche dense) pour empêcher notre réseau de faire du sur-apprentissage. En effet avec ces extinction de neurones, on entraîne virtuellement un forêt de réseaux tels que le réseau complet une fois entraîné sera la décision moyenne de cette forêt.\n",
    "Pour ce classifieur, nous avions accès à trois hyper-paramètres : le taux d’apprentissage, la taille du mini batch et le nombre de couches de convolutions.\n",
    "\n",
    "Pour la taille du mini batch, nous ne pouvons pas faire varier énormément car si celle ci devient trop importante le batch ne peut plus être chargé en mémoire. Nous avons donc testé avec des tailles de 25 et 10. Le taux d'apprentissage initial a été paramétré à 1e-1, 1e-3 et 1e-5. Nous avons comparé d’un autre côté l’apprentissage via 2 et 3 couches de convolution.\n",
    "\n",
    "Ci dessous la comparaison entre les deux tailles de mini batch:\n",
    "\n",
    "__Batch size = 10__\n",
    "![\"Batch size = 10\"](Graphs/2c10bs3lr.png)\n",
    "__Batch size = 25__\n",
    "![\"Batch size = 25\"](Graphs/2c25bs3lr.png)\n",
    "\n",
    "Ci dessous la comparaison entre deux et trois couches convolutives:\n",
    "\n",
    "__3 couches de convolution__\n",
    "![\"3 couches de convolution\"](Graphs/3c25bs3lr.png)\n",
    "__2 couches de convolution__\n",
    "![\"2 couches de convolution\"](Graphs/2c25bs3lr.png)\n",
    "\n",
    "Ci dessous la comparaison entre 3 valeurs de learning rate:\n",
    "\n",
    "__Learning rate = 1e-1__\n",
    "![\"Learning rate = 1e-1\"](Graphs/2c25bs1lr.png)\n",
    "__Learning rate = 1e-5__\n",
    "![\"Learning rate = 1e-5\"](Graphs/2c25bs5lr.png)\n",
    "__Learning rate = 1e-3__\n",
    "![\"Learning rate = 1e-3\"](Graphs/2c25bs3lr.png)\n",
    "\n",
    "On observe que le batch size de 10 permet un apprentissage plus constant, les écarts entre validation et entraînement sont moins importants. On observe aussi des meilleurs résultats sur les 10 premières itérations. Les mêmes effets sont visibles à l’ajout d’une troisième couche de convolution.\n",
    "Le learning rate lui à des effets bien plus visibles, un learning rate très grand fait diverger instantanément le réseau et le gradient devient très grand. Le learning rate de 1e-5 lui donne une courbe très lisse avec une précision égale au terme des 50 itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliographie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
