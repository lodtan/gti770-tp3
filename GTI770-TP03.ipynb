{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 3 : Machines à vecteurs de support et réseaux neuronaux\n",
    "#### Département du génie logiciel et des technologies de l’information\n",
    "\n",
    "| Étudiants             | LEMARCHANT HUGO - LEMH03039705 * TAN ELODIE - TANE25619607 * JACQUES-SYLVAIN LECOINTRE LECJ19128301|\n",
    "|-----------------------|---------------------------------------------------------|\n",
    "| Cours                 | GTI770 - Systèmes intelligents et apprentissage machine |\n",
    "| Session               | Automne 2018                                            |\n",
    "| Groupe                | C                                                       |\n",
    "| Numéro du laboratoire | 02                                                      |\n",
    "| Professeur            | Prof. Hervé Lombaert                                    |\n",
    "| Chargé de laboratoire | Pierre-Luc Delisle                                      |\n",
    "| Date                  | 31/10/2018                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use(\"Agg\")\n",
    "from time import time\n",
    "from IPython.display import Markdown\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score ,confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vectors = pd.read_csv('galaxy_feature_vectors.csv', delimiter = ',', header=None)\n",
    "labels = pd.read_csv('galaxy_label_data_set.csv', delimiter = ',')\n",
    "X_galaxy = pd.read_csv('galaxy_feature_vectors.csv', delimiter = ',', header=None).values[:,0:-1]\n",
    "Y_galaxy = pd.read_csv('galaxy_feature_vectors.csv', delimiter = ',', header=None).values[:,-1:].astype(int).flatten()\n",
    "Xg_train, Xg_test, Yg_train, Yg_test = train_test_split(X_galaxy, Y_galaxy, test_size=0.20, random_state=42, stratify=Y_galaxy)\n",
    "\n",
    "# On normalise nos features\n",
    "scaler = StandardScaler()\n",
    "Xg_train = scaler.fit_transform(Xg_train)\n",
    "Xg_test = scaler.fit_transform(Xg_test)\n",
    "# Création d'array pour stocker l'accuracy et le score f1 pour les différents nombres d'itération\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "accuracies_npercep = []\n",
    "f1_scores_npercep = []\n",
    "accuracies_layers = []\n",
    "f1_scores_layers = []\n",
    "accuracies_learning = []\n",
    "f1_scores_learning = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layers Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = 100\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 60\n",
    "n_hidden_1 = 100 # 1st layer number of neurons\n",
    "n_hidden_2 = 100 # 2nd layer number of neurons\n",
    "n_hidden_3 = 50 # 3rd layer number of neurons\n",
    "n_hidden_4 = 2 # 4th layer number of neurons\n",
    "num_input = 75\n",
    "num_classes = 2\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(n_hidden_1))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(n_hidden_2))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='no_dropout/learning_1E-3', batch_size=batch, histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "model.fit(Xg_train, Yg_train, epochs=n_epochs, batch_size=batch, validation_data=(Xg_test, Yg_test),\n",
    "          callbacks=[tensorboard])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(Xg_test, Yg_test)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "predictions = model.predict(Xg_test)\n",
    "print(predictions)\n",
    "Yg_pred = model.predict_classes(Xg_test)\n",
    "    \n",
    "f1 = f1_score(Yg_test, Yg_pred, average='weighted')\n",
    "print(\"f1 score: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs = 6\n",
    "cache_size=2048\n",
    "k=10\n",
    "svc = SVC(cache_size=cache_size)\n",
    "\n",
    "X_valid=Xg_train\n",
    "Y_valid=Yg_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recherche par grille SVM lineaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:  6.5min finished\n",
      "[Parallel(n_jobs=6)]: Done  40 out of  40 | elapsed:  6.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR : The best hyperparameters are {'C': 10, 'class_weight': 'balanced', 'gamma': 'scale', 'kernel': 'linear'} with a score of 0.94\n"
     ]
    }
   ],
   "source": [
    "def grid_search_linear(X_valid, Y_valid,k, jobs, cache_size):\n",
    "\n",
    "    svc = SVC(cache_size=cache_size)\n",
    "\n",
    "    param_grid_linear = {'kernel': ['linear'], 'C': [10 ** (-3), 10 ** (-1), 1, 10],'class_weight': ['balanced'],'gamma': ['scale']}\n",
    "\n",
    "    grid_linear = GridSearchCV(svc, param_grid=param_grid_linear, cv=k, n_jobs=jobs, scoring='accuracy', verbose=4)\n",
    "    grid_linear.fit(X_valid, Y_valid)\n",
    "\n",
    "    print(\"LINEAR : The best hyperparameters are %s with a score of %0.2f\" % (grid_linear.best_params_, grid_linear.best_score_))\n",
    "    \n",
    "    return grid_linear\n",
    "\n",
    "grid_linear = grid_search_linear(X_valid, Y_valid, k, jobs, cache_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy (Linear SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>0.925181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.940115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.940559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.940633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        precision\n",
       "C                \n",
       "0.001    0.925181\n",
       "0.100    0.940115\n",
       "1.000    0.940559\n",
       "10.000   0.940633"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(grid_linear.cv_results_['mean_test_score'],index=[x['C'] for x  in grid_linear.cv_results_['params']],columns=['precision'])\n",
    "df.index.name='C'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fiting time (Linear SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temps moyen du fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>36.547501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>19.690940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>34.404641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>90.281601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        temps moyen du fit\n",
       "C                         \n",
       "0.001            36.547501\n",
       "0.100            19.690940\n",
       "1.000            34.404641\n",
       "10.000           90.281601"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lin=pd.DataFrame(grid_linear.cv_results_['mean_fit_time'],index=[x['C'] for x  in grid_linear.cv_results_['params']],columns=['temps moyen du fit'])\n",
    "df_lin.index.name='C'\n",
    "df_lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recherche par grille SVM non-lineaire fonction noyau RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=6)]: Done  86 tasks      | elapsed: 32.0min\n",
      "[Parallel(n_jobs=6)]: Done 160 out of 160 | elapsed: 54.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF : The best hyperparameters are {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'} with a score of 0.95\n"
     ]
    }
   ],
   "source": [
    "def grid_search_rbf(X_valid, Y_valid,k, jobs, cache_size):\n",
    "\n",
    "    svc = SVC(cache_size=cache_size)\n",
    "    param_grid_rbf = {'kernel': ['rbf'], 'C': [10 ** (-3), 10 ** (-1), 1, 10], 'gamma': [10 ** (-3), 10 ** (-1), 1, 10]}\n",
    "    grid_rbf = GridSearchCV(svc, param_grid=param_grid_rbf, cv=k, n_jobs=jobs, scoring='accuracy', verbose=4)\n",
    "\n",
    "    grid_rbf.fit(X_valid, Y_valid)\n",
    "\n",
    "    print(\"RBF : The best hyperparameters are %s with a score of %0.2f\" % (grid_rbf.best_params_, grid_rbf.best_score_))\n",
    "\n",
    "    return grid_rbf\n",
    "\n",
    "grid_rbf = grid_search_rbf(X_valid, Y_valid, k, jobs, cache_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAGoCAYAAADo5YQfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBl5JkkbIwCtJ0ggZeCVJGqH/C/i7kOOOg0APAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.100</th>\n",
       "      <th>1.000</th>\n",
       "      <th>10.000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>0.519074</td>\n",
       "      <td>0.519074</td>\n",
       "      <td>0.519074</td>\n",
       "      <td>0.519074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.905072</td>\n",
       "      <td>0.894869</td>\n",
       "      <td>0.519074</td>\n",
       "      <td>0.519074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.938637</td>\n",
       "      <td>0.944773</td>\n",
       "      <td>0.614372</td>\n",
       "      <td>0.519074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.952166</td>\n",
       "      <td>0.945217</td>\n",
       "      <td>0.640766</td>\n",
       "      <td>0.519074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              \n",
       "gamma     0.001     0.100     1.000     10.000\n",
       "C                                             \n",
       "0.001   0.519074  0.519074  0.519074  0.519074\n",
       "0.100   0.905072  0.894869  0.519074  0.519074\n",
       "1.000   0.938637  0.944773  0.614372  0.519074\n",
       "10.000  0.952166  0.945217  0.640766  0.519074"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas = [x['gamma'] for x  in grid_rbf.cv_results_['params']]\n",
    "C_values=[x['C'] for x  in grid_rbf.cv_results_['params']]\n",
    "fit_time = grid_rbf.cv_results_['mean_fit_time']\n",
    "accuracies=grid_rbf.cv_results_['mean_test_score']\n",
    "\n",
    "C_range=[10 ** (-3), 10 ** (-1), 1, 10]\n",
    "gamma_range=[10 ** (-3), 10 ** (-1), 1, 10]\n",
    "\n",
    "grid=grid_rbf\n",
    "# We extract just the scores\n",
    "scores = grid.cv_results_['mean_test_score']\n",
    "scores = np.array(scores).reshape(len(C_range), len(gamma_range))\n",
    "\n",
    "# Make a nice figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplots_adjust(left=0.15, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest',cmap='Greens' )\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.title('Accuracy  pour SVM non lineaire avec fonction noyau RBF SVM')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.show()\n",
    "\n",
    "df=pd.DataFrame(list(zip(gammas,accuracies,C_values)),columns=['gamma','','C'])\n",
    "df.pivot_table(columns=['gamma'],index=['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fiting time (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\"></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.100</th>\n",
       "      <th>1.000</th>\n",
       "      <th>10.000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.001</th>\n",
       "      <td>108.665604</td>\n",
       "      <td>105.084420</td>\n",
       "      <td>100.575474</td>\n",
       "      <td>98.379786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>48.784804</td>\n",
       "      <td>47.269562</td>\n",
       "      <td>104.112872</td>\n",
       "      <td>108.335408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>27.710953</td>\n",
       "      <td>47.751589</td>\n",
       "      <td>108.571365</td>\n",
       "      <td>104.675441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>16.029991</td>\n",
       "      <td>42.508023</td>\n",
       "      <td>87.534814</td>\n",
       "      <td>87.072186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      \n",
       "gamma       0.001       0.100       1.000       10.000\n",
       "C                                                     \n",
       "0.001   108.665604  105.084420  100.575474   98.379786\n",
       "0.100    48.784804   47.269562  104.112872  108.335408\n",
       "1.000    27.710953   47.751589  108.571365  104.675441\n",
       "10.000   16.029991   42.508023   87.534814   87.072186"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(list(zip(gammas,fit_time,C_values)),columns=['gamma','','C'])\n",
    "df.pivot_table(columns=['gamma'],index=['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du modele SVM C=10 Gamma=0.001 RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9603784742755765 F1 score: 0.9603766424673736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1558,   69],\n",
       "       [  65, 1690]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(**grid_rbf.best_params_)\n",
    "clf.fit(Xg_train, Yg_train) \n",
    "Yg_pred = clf.predict(Xg_test)\n",
    "\n",
    "acc = accuracy_score(Yg_test, Yg_pred)\n",
    "f1 = f1_score(Yg_test, Yg_pred, average='weighted') \n",
    "print('accuracy : {0} F1 score: {1}'.format(acc,f1))\n",
    "confusion_matrix(Yg_test,Yg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9551680437288755\n"
     ]
    }
   ],
   "source": [
    "K=10\n",
    "X_normalized = scaler.fit_transform(X_galaxy)\n",
    "clf = SVC(**grid_rbf.best_params_)\n",
    "\n",
    "scores = cross_val_score(clf, X_normalized, Y_galaxy, cv=K)\n",
    "print(sum(scores)/K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(device_lib.list_local_devices())\n",
    "\n",
    "class NetCNN:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        # initialize the model\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    " \n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "        # first set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(20, (5, 5), padding=\"same\",input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # second set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(50, (5, 5), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # third set of CONV => RELU => POOL layers\n",
    "        model.add(Conv2D(20, (5, 5), padding=\"same\",input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dropout(0.3))\n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model\n",
    "    \n",
    "# initialize the data and labels\n",
    "print(\"[INFO] loading images...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "with open(\"..\\\\data\\\\data\\\\csv\\\\galaxy\\\\galaxy_label_data_set.csv\", 'rt') as csvFile:\n",
    "    reader = csv.reader(csvFile, delimiter=\",\")\n",
    "    # loop over the input images\n",
    "    for index, row in enumerate(reader):\n",
    "        if index in range(1,1000):\n",
    "            # load the image, pre-process it, and store it in the data list\n",
    "            file = \"..\\\\data\\\\data\\\\images\\\\\"+row[0]+\".jpg\"\n",
    "            image = cv2.imread(file)\n",
    "            image = cv2.resize(image, (212, 212))\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "            label = (0 if row[1] == 'smooth' else 1)\n",
    "            labels.append(label)\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "trainY = to_categorical(trainY, num_classes=2)\n",
    "testY = to_categorical(testY, num_classes=2)\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,height_shift_range=0.1, \n",
    "      shear_range=0.2, zoom_range=0.2,horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "EPOCHS = 50\n",
    "INIT_LR = 1e-3\n",
    "BS = 25\n",
    "\n",
    "# initialize the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "model = NetCNN.build(width=212, height=212, depth=3, classes=2)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=EPOCHS, verbose=1)\n",
    "\n",
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(\"..\\\\data\\\\tp3\\\\model.hdf5\")\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = EPOCHS\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Smooth/Spiral\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"Graphs\\\\3c25bs3lr.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Les Questions du notebook et celles de l'énoncé ne sont pas dans le même ordre, nous avons choisi de suivre l'ordre des questions proposé par l'énoncé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "### Présentation de la méthode de validation\n",
    "\n",
    "Nous avons opté pour une validation par hold-out. En effet toutes les autres méthodes sont gourmandes voir extrèmement gourmandes en temps de calcul pour les réseaux de neurones ou les SVM. Nous nous sommes donc contentés de séparer nos données en deux ensembles d'entraînement et de validation.\n",
    "Pour la SVM, nous avons procédé à une recherche par grille des meilleurs hyper-paramètres sur une fraction de notre enssemble d'entraînement et chaque couple d'hyper-paramètres est validé par 10-fold-CV. Une fois ces paramètres optimisés nous avons entraîné le modèle avec l'ensemble d'entraînement privé des données de validations puis avons fais les tests de validation.\n",
    "\n",
    "De manière équivalente pour les réseaux de neurones (et CNN) nous avons découpé notre ensemble en deux et avons fait nos tests d'hyper-paramètres avec ces ensemble d'entraînement et nous les comparons entre eux avec l'ensemble de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Description de la méthode de normalisation\n",
    "\n",
    "#### Réseaux de neurones\n",
    "\n",
    "Nous avons utilisé pour les réseaux de neurones la normalisation avec `StandardScaler()` de la librairie `scikitlearn`. Elle permet de transformer nos données en faisant en sorte d’avoir une moyenne de 0 et un écart type de 1, ce qui permet de d’éliminer en quelque sorte les outliers mais aussi de mettre à la même échelle l’ensemble des variables. Nous avons aussi essayé d’utiliser la fonction `normalize` de la librairie `sklearn.preprocessing`. Cependant, son utilisation n’était pas pertinente et les résultats étaient erronés car elle agit sur toute une ligne (et non pas une colonne comme `StandardScaler`). \n",
    "\n",
    "#### SVM\n",
    "\n",
    "Les SVM ont pour but de maximiser une marge formée par des vecteurs de support pour séparer des classes.Cette optmisation nécessite des calculs de distances entre vecteurs ce qui implique necessairement que l'algorithme est sensible à l'échelle entre les différentes caractéristiques (features).Une normalisation similaire a celle utilisée pour le MLP a été mise en oeuvre (classe `StandardScaler` de sklearn).Cette technique est également connue sous le nom de normalisation z-score (z=x−μσ) pour annuler l'effet de la moyenne et pour que les valeurs tombent dans un intervalle fixe [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Description du modèle élaboré et analyse de la phase d'entrainement\n",
    "\n",
    "#### Structure et choix du modèle d’apprentissage\n",
    "Nous avons créé deux couches cachées avec la fonction d’activation **ReLU**. Elle est beaucoup plus efficace que la fonction $tanh$, et c’est une fonction simple à dériver. De même, la fonction $Sigmoid$ prend beaucoup de temps pour converger et d’autres problèmes existent pour cette fonction comme le problème du *vanishing gradient*.  Nous n’utilisons pas $Heaviside$ car celle-ci n’est pas dérivable.\n",
    "\n",
    "De plus, nous avons choisi d’effectuer une descente de gradient par mini-batch. Il s’agit d’un compromis entre la descente stochastique de gradient qui calcule et rétro-propage l’erreur sur pour chaque observation et l’apprentissage par batch ou . La variation de la taille des lots (batchs) permet de faire varier ce compromis. Cependant nous ne faisons pas varier la taille des lots dans ce laboratoire et l’avons fixée à 100.\n",
    "\n",
    "A la sortie de chaque couche cachée, nous avons utilisé `BatchNormalisation()` et `Dropout()`. `BatchNormalisation()` permet au réseau de neurones de converger plus rapidement (mais pas forcément mieux) en normalisant chaque $batch$ comme avec `StandardScaler()`. En effet, lorsque nous ajustons les poids, l'echelle des données change et ne sont plus normalisées.  \n",
    "\n",
    "`Dropout()` permet de désactiver de façon aléatoire certains neurones d’une couche. Cela permet d’éviter le sur-apprentissage et de rendre le réseau plus robuste. Nous avons donc choisi, pour chaque couche cachée, d'appliquer un taux de `dropout` de 0.2. Nous n’avons pas pris de valeurs plus élevées car cela impacte aussi le taux la précision.   \n",
    "\n",
    "Pour finir, nous appliquons pour la dernière couche de neurones la fonction d’activation `softmax` qui permet d’obtenir en sortie les probabilités correspondant à chaque classe. \n",
    "\n",
    "#### Fonction de coût\n",
    "\n",
    "Nous avons utilisé `sparse_categorical_crossentropy` pour la fonction de coût. Tout d’abord, `crossentropy` correspond bien évidemment à la $Cross-Entropy$. Elle permet d’être plus rapide contrairement à la fonction d’erreur quadratique car celle-ci est très coûteuse en temps de calcul pour une machine. De plus, `sparse_categorical_crossentropy` permet l’utilisation de la fonction sur des labels qui sont donnés dans une seule colonne (c’est à dire le numéro du label reçu), contrairement à `categorical_crossentropy` qui nous obligeait à les coder en `one-hot`. Nous n’avons pas utilisé `binary_crossentropy` pour la même raison. \n",
    "\n",
    "La figure ci-dessous montre le graphe tensorboad obtenue pour un MLP a deux couches cachées realisé dans le cadre de ce projet avec l'API de tensorflow directement.Les mesures de performances ont été réalisées avec keras par la suite.\n",
    "\n",
    "On peut y voir la rétro-propagation (GradientDescent) de la sortie vers les couches precedentes et le calcul de l'erreur et de la précision entre la couche de sortie et l'entrée du réseau.\n",
    "\n",
    "![\"Tensorboard graph\"](./tensorboard_mlp_2hidden_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### Etude de l'évolution temporelle de l'apprentissage\n",
    "\n",
    "Le sur-entrainement arrive généralement lorsque le processus de dsescente de gradient converge sur l’ensemble de validation converge vers un minimum local de la fonction de coût à travers les itérations (epochs). Dans nos graphiques, cette condition arrive aux alentours de 20 itérations pour un pas d'apprentissage fixé a 0.1. Cette valeur varie en fonction du pas d'apprentissage choisi (rapidité avec laquelle les poids sont mis a jour apres retro-propgation de l'erreur). Cependant, parmi nos trois ajustements de pas d'apprentissage (3, 0.1 et 1e-6), ce dernier montrait des meilleures performances pour 0.1.Cette observation  semble cohérente puisqu’un pas d'apprentissage trop élevé est moins stable et peut amener a des oscillations de la fonction de coût,alors q'un pas d'apprentissage trop faible prendra plus de temps (plus d'epochs) à converger vers un minimum local.Le nombre optimal d'epochs dans le cadre de nos conditions expérimentales semble donc autour de 20. En effet, après 20 epochs, le gain de precision pour l’ensemble de validation stagne,et la diminution du coût aussi. Nous obtenons un bon compromis qualité des predictions/temps d'entrainement avec 20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "### Matrice des expérimentations\n",
    "\n",
    "#### Impact du nombre d’epochs\n",
    "\n",
    "La généralisation du modèle est ce que l’on cherche à obtenir. Le modèle doit être capable de généraliser et donc de classifier correctement des  nouvelles données. Pour éviter le surajustement il faut donc être capable de déterminer à quel moment arrêter l'entraînement pour minimiser les erreurs de sur les données de test. Pour déterminer le nombre de cycles d’apprentissage (epochs), nous utilisons la base de validation.Cet hyperparamètre a un impact sur le sur-apprentissage et donc sur les performances générales du modele.\n",
    " \n",
    "Cette interruption prématurée (early-stop) est mise en œuvre en présentant les échantillons de la base de validation à chaque cycle d'entraînement et en utilisant la fonction d’erreur pour calculer l’erreur de ce cycle. Lorsque l’erreur de la base de validation ne diminue plus ou se met à augmenter, l’apprentissage est alors interrompu prématurement.\n",
    "\n",
    "![\"NbEpochs\"](Graphs/image.png)\n",
    "\n",
    "#### Impact du nombre de couches\n",
    "\n",
    "Sans couche cachée,la topologie ne peut séparer que des données séparables linéairement.l’ajout de couches permet de projeter les données dans un espace où les données deviennent linéairement séparables.\n",
    "\n",
    "Avec une deux couches  cachées, les frontières de décision peuvent théoriquement être d’une complexité arbitraire.Cependant,l'ajout d'une couche supplémentaire n'est pas toujours justifié et ne se traduit pas forcément en gain significatif de performance comme le montre la figure ci-dessous.\n",
    "\n",
    "![\"NbLayers\"](Graphs/image1.png)\n",
    "\n",
    "#### Impact du nombre de perceptrons dans les couches intermédiaires\n",
    "\n",
    "Le nombre de perceptrons dans les couches d'entrée et de sortie sont fixés et dépendent respectivement du nombre de dimensions (taille des vecteurs de caractéristiques) et du nombre de classes (2). Un perceptron implémente un hyperplan, Augmenter le nombre de perceptrons permet donc d’obtenir des régions de décision plus complexes constituées de plusieurs hyperplans.\n",
    "\n",
    "![\"NbPerceptrons\"](Graphs/image3.png)\n",
    "\n",
    "#### Impact du taux d’apprentissage (learning rate)\n",
    "\n",
    "Le taux d’apprentissage permet de contrôler la vitesse avec laquelle les poids sont ajustés lors de la descente de gradient.La retro-propagation de l’erreur permet de converger vers un minimum local de la fonction de coût.Si le taux d’apprentissage est trop élevé,la descente de gradient pourrait ne pas converger .Inversement ,un taux d’apprentissage trop faible ralenti la vitesse de convergence de la descente de gradient et donc peut être un obstacle pour des réseaux profonds qui nécessitent beaucoup de calculs.\n",
    "\n",
    "![\"LearningRate\"](Graphs/image2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avec drop-out : Loss et Accuracies\n",
      "                            loss     acc  val_loss  val_acc\n",
      "avec paramètres initiaux  0.2496  0.9063    0.1959   0.9370\n",
      "100 epochs                0.2116  0.9204    0.1544   0.9503\n",
      "200 epochs                0.1927  0.9283    0.1426   0.9548\n",
      "400 epochs                0.1391  0.9472    0.1100   0.9622\n",
      "600 epochs                0.1231  0.9547    0.1086   0.9651\n",
      "                       loss     acc  val_loss  val_acc\n",
      "100, 50 perceptrons  0.2153  0.9166    0.1691   0.9385\n",
      "50, 100              0.2213  0.9176    0.1586   0.9447\n",
      "200, 25              0.2022  0.9233    0.1646   0.9450\n",
      "50, 25               0.2784  0.8918    0.2254   0.9199\n",
      "                       loss     acc  val_loss  val_acc\n",
      "100, 50 perceptrons  0.2153  0.9166    0.1691   0.9385\n",
      "50, 100              0.2213  0.9176    0.1586   0.9447\n",
      "200, 25              0.2022  0.9233    0.1646   0.9450\n",
      "50, 25               0.2784  0.8918    0.2254   0.9199\n",
      "                       loss     acc  val_loss  val_acc\n",
      "learning rate: 3     0.1061  0.9607    0.0942   0.9663\n",
      "learning rate: 0.1   0.0961  0.9649    0.0869   0.9687\n",
      "learning rate: 1E-6  1.2686  0.4354    1.1457   0.4243\n",
      "\n",
      "\n",
      "Sans drop-out : Loss, Accuracies avec F1-mesures\n",
      "                            loss     acc  val_loss  val_acc  test_accuracy  \\\n",
      "avec paramètres initiaux  0.1577  0.9415    0.1729   0.9468       0.946777   \n",
      "100 epochs                0.1384  0.9473    0.1387   0.9500       0.951804   \n",
      "200 epochs                0.1134  0.9590    0.1213   0.9500       0.957717   \n",
      "400 epochs                0.0955  0.9650    0.1093   0.9600       0.963631   \n",
      "\n",
      "                          F1 score  \n",
      "avec paramètres initiaux  0.946785  \n",
      "100 epochs                0.951793  \n",
      "200 epochs                0.957714  \n",
      "400 epochs                0.963626  \n",
      "                       loss     acc  val_loss  val_acc  test_accuracy  \\\n",
      "100, 50 perceptrons  0.1675  0.9382    0.1640   0.9527       0.952691   \n",
      "50, 100              0.1722  0.9372    0.1792   0.9409       0.940863   \n",
      "200, 25              0.1579  0.9422    0.1629   0.9438       0.943820   \n",
      "50, 25               0.1777  0.9333    0.1861   0.9400       0.939976   \n",
      "\n",
      "                     F1 score  \n",
      "100, 50 perceptrons  0.952691  \n",
      "50, 100              0.940826  \n",
      "200, 25              0.943840  \n",
      "50, 25               0.939983  \n",
      "                       loss     acc  val_loss  val_acc  test_accuracy  \\\n",
      "100, 50 perceptrons  0.1675  0.9382    0.1640   0.9527       0.952691   \n",
      "50, 100              0.1722  0.9372    0.1792   0.9409       0.940863   \n",
      "200, 25              0.1579  0.9422    0.1629   0.9438       0.943820   \n",
      "50, 25               0.1777  0.9333    0.1861   0.9400       0.939976   \n",
      "\n",
      "                     F1 score  \n",
      "100, 50 perceptrons  0.952691  \n",
      "50, 100              0.940826  \n",
      "200, 25              0.943840  \n",
      "50, 25               0.939983  \n",
      "                       loss     acc  val_loss  val_acc  test_accuracy  \\\n",
      "learning rate: 3     0.0661  0.9752    0.3223   0.9119       0.911886   \n",
      "learning rate: 0.1   0.0232  0.9905    0.1833   0.9539       0.953873   \n",
      "learning rate: 1E-6  0.9533  0.5129    0.9114   0.5269       0.526907   \n",
      "\n",
      "                     F1 score  \n",
      "learning rate: 3     0.911616  \n",
      "learning rate: 0.1   0.953839  \n",
      "learning rate: 1E-6  0.527057  \n"
     ]
    }
   ],
   "source": [
    "print(\"Avec drop-out : Loss et Accuracies\")\n",
    "\n",
    "ar = np.array([[0.2496, 0.9063, 0.1959, 0.9370], \n",
    "                  [0.2116, 0.9204, 0.1544, 0.9503], \n",
    "                  [0.1927, 0.9283, 0.1426, 0.9548], \n",
    "                  [0.1391, 0.9472, 0.1100, 0.9622],\n",
    "                  [0.1231, 0.9547, 0.1086, 0.9651]])\n",
    "dfepoch = pd.DataFrame(ar, \n",
    "                      index = ['avec paramètres initiaux','100 epochs', '200 epochs', '400 epochs', '600 epochs'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "\n",
    "print(dfepoch)\n",
    "\n",
    "ar = np.array([[0.2153, 0.9166, 0.1691, 0.9385], \n",
    "                  [0.2213, 0.9176, 0.1586, 0.9447],\n",
    "                  [0.2022, 0.9233, 0.1646, 0.9450],\n",
    "                  [0.2784, 0.8918, 0.2254, 0.9199]])\n",
    "dfpercep = pd.DataFrame(ar, \n",
    "                      index = ['100, 50 perceptrons', '50, 100', '200, 25', '50, 25'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "print(dfpercep)\n",
    "\n",
    "ar = np.array([[0.2121, 0.9194, 0.1641, 0.9420], \n",
    "                  [0.2393, 0.9094, 0.1699, 0.9420], \n",
    "                  [0.2579, 0.8981, 0.1821, 0.9323]])\n",
    "dflayers = pd.DataFrame(ar, \n",
    "                      index = ['3 couches cachées', '4 couches', '5 couches'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "print(dfpercep)\n",
    "\n",
    "ar = np.array([[0.1061, 0.9607, 0.0942, 0.9663], \n",
    "                  [0.0961, 0.9649, 0.0869, 0.9687], \n",
    "                  [1.2686, 0.4354, 1.1457, 0.4243]])\n",
    "dflearning = pd.DataFrame(ar, \n",
    "                      index = ['learning rate: 3', 'learning rate: 0.1', 'learning rate: 1E-6'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc'])\n",
    "print(dflearning)\n",
    "\n",
    "print(\"\\n\\nSans drop-out : Loss, Accuracies avec F1-mesures\")\n",
    "\n",
    "ar = np.array([[0.1577, 0.9415, 0.1729, 0.9468, 0.9467770548912987, 0.9467854026859117], \n",
    "                  [0.1384, 0.9473, 0.1387, 0.95, 0.9518036663638002, 0.9517932604948441\n",
    "], \n",
    "                  [0.1134, 0.9590, 0.1213, 0.95, 0.9577173269196843, 0.9577138280605294], \n",
    "                  [0.0955, 0.9650, 0.1093, 0.96, 0.9636309874755683, 0.963626117614296]])\n",
    "dfepoch = pd.DataFrame(ar, \n",
    "                      index = ['avec paramètres initiaux','100 epochs', '200 epochs', '400 epochs'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc', 'test_accuracy', 'F1 score'])\n",
    "\n",
    "print(dfepoch)\n",
    "\n",
    "ar = np.array([[0.1675, 0.9382, 0.1640, 0.9527, 0.9526907155529273, 0.9526907155529273], \n",
    "                  [0.1722, 0.9372, 0.1792, 0.9409, 0.9408633943354145, 0.9408263855468233],\n",
    "                  [0.1579, 0.9422, 0.1629, 0.9438, 0.9438202247191011, 0.9438398613167942],\n",
    "                  [0.1777, 0.9333, 0.1861, 0.9400, 0.939976345252032, 0.9399831073373335]])\n",
    "dfpercep = pd.DataFrame(ar, \n",
    "                      index = ['100, 50 perceptrons', '50, 100', '200, 25', '50, 25'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc', 'test_accuracy', 'F1 score'])\n",
    "print(dfpercep)\n",
    "\n",
    "ar = np.array([[0.1513, 0.9424, 0.1448, 0.9503, 0.9503252512248291, 0.9503152269164241], \n",
    "                  [0.1437, 0.9451, 0.1497, 0.9471, 0.9470727379190929, 0.9470733265279817], \n",
    "                  [0.1470, 0.9426, 0.1617, 0.9432, 0.9432288586635127, 0.9432071322488779]])\n",
    "dflayers = pd.DataFrame(ar, \n",
    "                      index = ['3 couches cachées', '4 couches', '5 couches'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc', 'test_accuracy', 'F1 score'])\n",
    "print(dfpercep)\n",
    "\n",
    "ar = np.array([[0.0661, 0.9752, 0.3223, 0.9119, 0.9118864576115825, 0.9116155733993972], \n",
    "                  [0.0232, 0.9905, 0.1833, 0.9539, 0.9538734475583596, 0.9538392862815737], \n",
    "                  [0.9533, 0.5129, 0.9114, 0.5269, 0.5269071556702652, 0.5270565773067073]])\n",
    "dflearning = pd.DataFrame(ar, \n",
    "                      index = ['learning rate: 3', 'learning rate: 0.1', 'learning rate: 1E-6'], \n",
    "                      columns = ['loss', 'acc', 'val_loss', 'val_acc', 'test_accuracy', 'F1 score'])\n",
    "print(dflearning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "### Présentation de la méthode de recherche des meilleurs hyper paramètres de SVM\n",
    "\n",
    "#### Méthode utilisée\n",
    "\n",
    "Nous avons utilisé la méthode de recherche par grille ( `GridSearch` avec sklearn ) qui permet de tester chaque combinaison d’hyper paramètres définie et de sélectionner celle qui maximise un certain critère. Dans le cadre de ce TP, nous avons choisi le critère de précision pour sélectionner les hyper-parametres C et Gamma.Cette recherche par grille s’effectue sur un base de données de validation (sous ensemble de la base de données  d'entraînement) dédiée à cette recherche d'hyper paramètres. De plus,la recherche par gille a été réalisée avec une validation croisée ce qui permet d’avoir une plus grande certitude quant au choix du modèle SVM.\n",
    "\n",
    "#### Résultats\n",
    "\n",
    "Au total, 16 combinaisons d’hyper paramètres C et Gamma ont été testées pour le SVM non linéaire ce qui donne 160 itérations avec une 10-fold validation croisée.le temps d'exécution total de cette recherche par grille est de 55.5 minutes pour 13200 vecteurs de dimension 75.\n",
    "\n",
    "Pour le SVM linéaire 4 combinaisons ont été testées pour différentes valeurs de C soit 40 itérations au total.La durée totale est de 6.5 minutes.\n",
    "\n",
    "La normalisation des données est primordiale pour que les modèles de type SVM puissent avoir des temps d'exécution acceptables.\n",
    "\n",
    "L'hyper-paramètre C est le compromis entre la pertinence des classifications (sur les données d'entraînement) et la taille de la marge (généralisation aux données de test).Idéalement, SVM souhaite maximiser la marge entre les hyper-plans séparateurs (formdés par les vecteurs de support) tout en ayant des classifications parfaites.\n",
    "\n",
    "C=10 et Gamma 0.001 est la meilleure combinaison d'hyper-paramètres obtenue pour un SVM non-linéaire avec une fonction noyau de type RBF.La précision obtenue est de 0.9532 avec ces paramètres.\n",
    "\n",
    "Pour un SVM linéaire,  le meilleur hyper-paramètre obtenu est pour C=10 avec une précision de 0.942629.Pour les autres valeurs , les précisions varient peu mais le temps d'exécution est deux a trois fois moins long lorsque C diminue (mean_fit_time retourné par gridsearch).\n",
    "\n",
    "La plus grande valeur de C=10 a donc été retenue , ce qui assure donc peu d’erreurs de classifications sur les données d’entrainements et cette non-tolérance semble bien se généraliser sur les données de test.\n",
    "\n",
    "\n",
    "#### Impact des hyperparamètres et utilité\n",
    "\n",
    "L’approche SVM (linéaire) utilisée dans ce TP consiste à maximiser la marge entre les hyperplans qui séparent les classes formés par les vecteurs de support de ces classes. Maximiser cette marge de l’hyperplan M = 2 / ||w||  revient à minimiser la fonction de coût L(w) = ||w||^2/ 2 (critère d’optimisation quadratique) avec la contrainte que chaque échantillon de du vecteur de test soit correctement classifié.\n",
    " \n",
    "Dans le cas où les échantillons sont non séparables, un hyper-paramètre C est introduit qui est le compromis de la marge poreuse (soft-margin) .La fonction de coût à minimiser devient alors sous contrainte.La tolérance aux erreurs est plus ou moins accentuée avec le paramètre C. Plus C est grand plus la tolérance aux mauvaises classifications est basse,ce qui implique une plus petite marge.\n",
    " \n",
    "Dans le cas ou les classes ne sont pas séparables linéairement,les vecteurs d’entrée sont projetés dans un espace de plus grande dimension avec une transformation non linéaire à l’aide d’une fonction noyau de type RBF qui ont des propriétés permettant de simplifier le calcul (astuce du noyau).\n",
    " \n",
    "L‘apprentissage devient un problème d’optimisation quadratique sous contraintes (avec méthode Lagrangienne et un multiplicateur Lagrangien) ce qui rend ces modèles sensibles a la taille des données.\n",
    " \n",
    "Dan ce cas il y  a deux hyperparamètres C et Gamma.Gamma contrôle la variance des fonctions noyaux (inverse de sigma) Plus gamma augmente plus les rayons autour des de chaque points seront resserrés ce qui entraîne du sur-apprentissage.A l’inverse , un Gamma trop petit donnera une mauvaise généralisation et un sous-apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "### Discussion de l'impact de la taille des données\n",
    "\n",
    "Pour des primitives intéressantes, une taille plus importante de données est bien sûr bénéfique car cela nous permettra de mieux généraliser notre problème, cependant cela va rallonger le temps d'entraînement de notre algorithme.\n",
    "\n",
    "Pour les SVM , la complexité de l’algorithme est quadratique avec la taille des données : O(nombre de caractéristiques x nombre d’observation^2), augmenter la taille des données pourrait rendre l’algorithme trop coûteux en mémoire et en CPU.Pour ce faire les prédispositions suivantes ont été prises compte-tenu de la taille des données :\n",
    "\n",
    "- L'utilisation appropriée du cache (2GB)\n",
    "- Normalisation des données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "### Formulation des recommandations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "### Améliorations possibles\n",
    "\n",
    "La préparation des données pourrait permmettre une amélioration des performances en réduisant le bruit et les données aberrantes.\n",
    "\n",
    "Effectuer une meilleure normalisation avant d'entraîner les classifieurs comme une mise a l'échelle Min-Max.\n",
    "\n",
    "Nous pourrions aussi faire une réduction de dimensionnalité pour le SVM puisque la complexité de l’algorithme dépend directement du nombre de features donc cela pourrait être une piste d’amélioration pour nos classificateurs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Bonus\n",
    "\n",
    "Le réseau de neurones convolutif est un réseau de neurones classique à la différence que les neurones de la couche de convolution partagent le même poids, qui est en fait une fenêtre de convolution. C’est à dire dans notre cas une matrice 5x5 de poids. De cette manière chaque neurone correspond à un déplacement de la matrice sur l’image (un déplacement de la fenêtre de convolution). L’objectif pour ces couches est de trouver les poids de cette matrice tels qu’ils minimisent la fonction de coût (ici binary_crossentropy). Ensuite après cette couche de convolution, nous faisons un maxPooling qui revient à considérer un cadre de 2x2 neurones et de synthétiser cette fenêtre en la valeur maximale présente dans cette fenêtre (on utilise le max plutôt que la moyenne car la littérature montre que le maximum décrit mieux la fenêtre que la moyenne).\n",
    "\n",
    "Nos de convolutions suivies de maxPooling activées avec ReLU, une couche dense (ou fully-connected) avec la même fonction d’activation ReLU et enfin une couche de décision de deux neurones avec une activation softMax pour traduire les résultats en probabilités.\n",
    "\n",
    "Nous avons de plus ajouté un dropout sur nos différentes couches (20% et 30% pour les couches de convolution et la couche dense) pour empêcher notre réseau de faire du sur-apprentissage. En effet avec ces extinction de neurones, on entraîne virtuellement un forêt de réseaux tels que le réseau complet une fois entraîné sera la décision moyenne de cette forêt.\n",
    "Pour ce classifieur, nous avions accès à trois hyper-paramètres : le taux d’apprentissage, la taille du mini batch et le nombre de couches de convolutions.\n",
    "\n",
    "Pour la taille du mini batch, nous ne pouvons pas faire varier énormément car si celle ci devient trop importante le batch ne peut plus être chargé en mémoire. Nous avons donc testé avec des tailles de 25 et 10. Le taux d'apprentissage initial a été paramétré à 1e-1, 1e-3 et 1e-5. Nous avons comparé d’un autre côté l’apprentissage via 2 et 3 couches de convolution.\n",
    "\n",
    "Ci dessous la comparaison entre les deux tailles de mini batch:\n",
    "\n",
    "__Batch size = 10__\n",
    "![\"Batch size = 10\"](Graphs/2c10bs3lr.png)\n",
    "__Batch size = 25__\n",
    "![\"Batch size = 25\"](Graphs/2c25bs3lr.png)\n",
    "\n",
    "Ci dessous la comparaison entre deux et trois couches convolutives:\n",
    "\n",
    "__3 couches de convolution__\n",
    "![\"3 couches de convolution\"](Graphs/3c25bs3lr.png)\n",
    "__2 couches de convolution__\n",
    "![\"2 couches de convolution\"](Graphs/2c25bs3lr.png)\n",
    "\n",
    "Ci dessous la comparaison entre 3 valeurs de learning rate:\n",
    "\n",
    "__Learning rate = 1e-1__\n",
    "![\"Learning rate = 1e-1\"](Graphs/2c25bs1lr.png)\n",
    "__Learning rate = 1e-5__\n",
    "![\"Learning rate = 1e-5\"](Graphs/2c25bs5lr.png)\n",
    "__Learning rate = 1e-3__\n",
    "![\"Learning rate = 1e-3\"](Graphs/2c25bs3lr.png)\n",
    "\n",
    "On observe que le batch size de 10 permet un apprentissage plus constant, les écarts entre validation et entraînement sont moins importants. On observe aussi des meilleurs résultats sur les 10 premières itérations. Les mêmes effets sont visibles à l’ajout d’une troisième couche de convolution.\n",
    "Le learning rate lui à des effets bien plus visibles, un learning rate très grand fait diverger instantanément le réseau et le gradient devient très grand. Le learning rate de 1e-5 lui donne une courbe très lisse avec une précision égale au terme des 50 itérations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliographie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
